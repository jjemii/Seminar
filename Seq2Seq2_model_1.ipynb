{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "H0o8PPgD63ZP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaILF2Y66mjv"
      },
      "source": [
        "# Summarizing Text with Caurtion corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOgbGhKS6mjx"
      },
      "source": [
        "\n",
        "To build our model we will use a two-layered bidirectional RNN with LSTMs on the input data and two layers, each with an LSTM using bahdanau attention on the target data. [Jaemin Cho's tutorial](https://github.com/j-min/tf_tutorial_plus/tree/master/RNN_seq2seq/contrib_seq2seq) for seq2seq was really helpful to get the code in working order because this is my first project with TensorFlow 1.1; some of the functions are very different from 1.0. The architecture for this model is similar to Xin Pan's and Peter Liu's, here's their [GitHub page.](https://github.com/tensorflow/models/tree/master/textsum)\n",
        "\n",
        "The sections of this project are:\n",
        "- Inspecting the Data\n",
        "- Preparing the Data\n",
        "- Building the Model\n",
        "- Training the Model\n",
        "- Making Our Own Summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZT8kWxkAKmL"
      },
      "source": [
        "Google connect to acess you the data stored on your drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSFNsM03ye63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "71be01ca-3210-4bda-d4c5-ce45f143691a"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 144465 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.22-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.22-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.22-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UGhX72o_XNr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "cfc6a952-9c61-48a8-9c4e-3df831662099"
      },
      "source": [
        "#need to install older version of tf\n",
        "!pip install tensorflow==1.15     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 47.1MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 56.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.30.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.2.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (49.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=6ff1531b474cc3da5396b83bc45cf16a022d30fcea1e2b5286f21a0b4f18a1ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw0keeCK6mjy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c6b7dbd-4de3-4c7c-fdbd-8864d0f87070"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w8w71ooATmY"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0o8PPgD63ZP"
      },
      "source": [
        "### Google Connect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7s5j-U46mj5"
      },
      "source": [
        "## Insepcting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o24fDOcG6mj6"
      },
      "source": [
        "news = pd.read_csv(\"/content/drive/Datafile.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv2B397Z6mj8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d9e319b-a0ea-4de5-9633-c0120a2ffa96"
      },
      "source": [
        "news.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rmt7bTH6mkA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "4ead564c-be6f-4e45-a5c8-72da0dff8044"
      },
      "source": [
        "news.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.independent.co.uk/news/world/europ...</td>\n",
              "      <td>Non-stunned halal and kosher meat not saleable...</td>\n",
              "      <td>Animals must be stunned prior to being killed ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://techxplore.com/news/2018-07-bacteria-p...</td>\n",
              "      <td>Solar cells powered by hybrid E. coli convert ...</td>\n",
              "      <td>Researchers in Canada have developed an innova...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://www.themalaymailonline.com/malaysia/art...</td>\n",
              "      <td>Southeast Asia unprepared for ISIS attacks say...</td>\n",
              "      <td>Southeast Asia is unprepared for the rapidly-r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://www.scotsman.com/business/companies/ret...</td>\n",
              "      <td>Speedy Hire rejects calls for merger with HSS</td>\n",
              "      <td>In addition to calls from Toscafund to oust it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://www.wsj.com/articles/transferwise-plans...</td>\n",
              "      <td>TransferWise to launch China services\\r\\n</td>\n",
              "      <td>TransferWise has announced plans to launch a c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>http://www.standard.co.uk/news/uk/londoners-at...</td>\n",
              "      <td>Pollution and other environmental factor linke...</td>\n",
              "      <td>A lack of Vitamin D, polluted air, exposure to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>http://www.vanitatis.elconfidencial.com/estilo...</td>\n",
              "      <td>Soft Silhouette is used to treat sagging skin</td>\n",
              "      <td>Soft Silhouette can be used to treat sagging j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>http://www.japantimes.co.jp/news/2016/05/02/re...</td>\n",
              "      <td>Japan's new law bans bias against disabilities</td>\n",
              "      <td>On 1 April 2016, a new Japanese law came into ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>https://www.msn.com/en-ph/foodanddrink/foodnew...</td>\n",
              "      <td>San Francisco restaurants serve up more co-wor...</td>\n",
              "      <td>Spacious, which offers a subscription-based se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>http://www.communitycare.co.uk/2017/07/20/coun...</td>\n",
              "      <td>UK councils tripled overseas hires of social ...</td>\n",
              "      <td>Difficulty attracting experienced social worke...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 url  ...                                               Text\n",
              "0  https://www.independent.co.uk/news/world/europ...  ...  Animals must be stunned prior to being killed ...\n",
              "1  https://techxplore.com/news/2018-07-bacteria-p...  ...  Researchers in Canada have developed an innova...\n",
              "2  http://www.themalaymailonline.com/malaysia/art...  ...  Southeast Asia is unprepared for the rapidly-r...\n",
              "3  http://www.scotsman.com/business/companies/ret...  ...  In addition to calls from Toscafund to oust it...\n",
              "4  http://www.wsj.com/articles/transferwise-plans...  ...  TransferWise has announced plans to launch a c...\n",
              "5  http://www.standard.co.uk/news/uk/londoners-at...  ...  A lack of Vitamin D, polluted air, exposure to...\n",
              "6  http://www.vanitatis.elconfidencial.com/estilo...  ...  Soft Silhouette can be used to treat sagging j...\n",
              "7  http://www.japantimes.co.jp/news/2016/05/02/re...  ...  On 1 April 2016, a new Japanese law came into ...\n",
              "8  https://www.msn.com/en-ph/foodanddrink/foodnew...  ...  Spacious, which offers a subscription-based se...\n",
              "9  http://www.communitycare.co.uk/2017/07/20/coun...  ...  Difficulty attracting experienced social worke...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFa2j3hVBzu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f5adafa-ca58-46fb-8b30-c1e9608fa8f2"
      },
      "source": [
        "#check columns\n",
        "column_names = news.columns\n",
        "print(column_names)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['url', 'Summary', 'Text'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbtLoD046mkD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "26efa4f9-ba19-4547-cb8f-a87b24b74a21"
      },
      "source": [
        "# Check for any nulls values\n",
        "news.isnull().sum()\n",
        "news.dropna(how='all')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.independent.co.uk/news/world/europ...</td>\n",
              "      <td>Non-stunned halal and kosher meat not saleable...</td>\n",
              "      <td>Animals must be stunned prior to being killed ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://techxplore.com/news/2018-07-bacteria-p...</td>\n",
              "      <td>Solar cells powered by hybrid E. coli convert ...</td>\n",
              "      <td>Researchers in Canada have developed an innova...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://www.themalaymailonline.com/malaysia/art...</td>\n",
              "      <td>Southeast Asia unprepared for ISIS attacks say...</td>\n",
              "      <td>Southeast Asia is unprepared for the rapidly-r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://www.scotsman.com/business/companies/ret...</td>\n",
              "      <td>Speedy Hire rejects calls for merger with HSS</td>\n",
              "      <td>In addition to calls from Toscafund to oust it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://www.wsj.com/articles/transferwise-plans...</td>\n",
              "      <td>TransferWise to launch China services\\r\\n</td>\n",
              "      <td>TransferWise has announced plans to launch a c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>https://www.irishtimes.com/business/hammond-ta...</td>\n",
              "      <td>UK chancellor to target offshore gambling comp...</td>\n",
              "      <td>UK Chancellor Philip Hammond is expected to in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>http://www.tmcnet.com/usubmit/2015/07/30/82257...</td>\n",
              "      <td>ALLOY Health platform chosen by Livongo Health...</td>\n",
              "      <td>Livongo Health empowers people who have chroni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>https://chemicalwatch.com/65818/furniture-flam...</td>\n",
              "      <td>Furniture flame retardant smoke toxicity to be...</td>\n",
              "      <td>The capacity for flame retardants (FRs) to pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>https://www.insurancebusinessmag.com/asia/news...</td>\n",
              "      <td>AXA launches solutions integration platform fo...</td>\n",
              "      <td>AXA Insurance has launched a one-stop, self-se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>http://www.euronews.com/2016/09/16/berlin-elec...</td>\n",
              "      <td>German Chancellors' position at risk as Berlin...</td>\n",
              "      <td>German Chancellor Angela Merkel's party is bra...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     url  ...                                               Text\n",
              "0      https://www.independent.co.uk/news/world/europ...  ...  Animals must be stunned prior to being killed ...\n",
              "1      https://techxplore.com/news/2018-07-bacteria-p...  ...  Researchers in Canada have developed an innova...\n",
              "2      http://www.themalaymailonline.com/malaysia/art...  ...  Southeast Asia is unprepared for the rapidly-r...\n",
              "3      http://www.scotsman.com/business/companies/ret...  ...  In addition to calls from Toscafund to oust it...\n",
              "4      http://www.wsj.com/articles/transferwise-plans...  ...  TransferWise has announced plans to launch a c...\n",
              "...                                                  ...  ...                                                ...\n",
              "39995  https://www.irishtimes.com/business/hammond-ta...  ...  UK Chancellor Philip Hammond is expected to in...\n",
              "39996  http://www.tmcnet.com/usubmit/2015/07/30/82257...  ...  Livongo Health empowers people who have chroni...\n",
              "39997  https://chemicalwatch.com/65818/furniture-flam...  ...  The capacity for flame retardants (FRs) to pro...\n",
              "39998  https://www.insurancebusinessmag.com/asia/news...  ...  AXA Insurance has launched a one-stop, self-se...\n",
              "39999  http://www.euronews.com/2016/09/16/berlin-elec...  ...  German Chancellor Angela Merkel's party is bra...\n",
              "\n",
              "[40000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m55yoyyc6mkG"
      },
      "source": [
        "# Remove null values and unneeded features\n",
        "\n",
        "news = news.dropna()\n",
        "news = news.drop(['url'], axis=1)\n",
        "news = news.reset_index(drop=True)\n",
        "news.head()\n",
        "\n",
        "# drop row, if values in Summary is missing. \n",
        "news.dropna(subset=['Summary'],inplace = True)\n",
        "news.dropna(subset=['Text'],inplace = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z054OKaL6mkM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "a9afb6db-06ac-4f0d-a7e6-c86ac604b5c7"
      },
      "source": [
        "# Inspecting some of the news\n",
        "for i in range(5):\n",
        "    print(\"news #\",i+1)\n",
        "    print(\"  the text is  :\",news.Text[i])\n",
        "    print(\"-------------\")\n",
        "    print(\"the summary is :\",news.Summary[i])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "news # 1\n",
            "  the text is  : Animals must be stunned prior to being killed in order for halal and kosher meat to be marketed with the official European Union (EU) organic label, the European Court of Justice has ruled. The judges argued that organic labelling reflects the highest animal welfare standards, and that stunning is integral to these as it significantly reduces suffering. The UK’s Food Standards Agency estimates that 88% of animals killed under halal methods are stunned. However, a minority of halal – and all kosher – meat is produced without stunning. The case was brought by a French animal welfare association in 2012.\r\n",
            "\n",
            "-------------\n",
            "the summary is : Non-stunned halal and kosher meat not saleable as organic: ECJ\n",
            "\n",
            "news # 2\n",
            "  the text is  : Researchers in Canada have developed an innovative solar cell which uses bacteria to convert light into energy. The team at the University of British Columbia developed a way of genetically engineering E.coli to produce large amounts of lycopene, a natural dye which bacteria use for photosynthesis. By coating the bacteria with a semiconducting substance and incorporating it into a battery cell, they were able to achieve a current density of 0.686 milliamps per square centimetre, which they say is the highest yet achieved by a biogenic solar cell.\n",
            "-------------\n",
            "the summary is : Solar cells powered by hybrid E. coli convert light to energy\n",
            "\n",
            "news # 3\n",
            "  the text is  : Southeast Asia is unprepared for the rapidly-rising threat of extremist attacks, warns think tank The Institute for Policy Analysis of Conflict (IPAC). It suggests the main danger is in the southern Philippines, where a few Islamic extremist groups have sworn allegiance to ISIS; the groups have links to other parts of the region, particularly Malaysia and Indonesia. ISIS has also endorsed a Philippines-based militant leader for Southeast Asia and widened its \"extremist recruitment pool\" in the region, opening up channels for international funding and communication. IPAC says as ISIS loses territory in Syria and Iraq, it raises the risk of revenge attacks in Southeast Asia. \n",
            "-------------\n",
            "the summary is : Southeast Asia unprepared for ISIS attacks says think-tank\n",
            "\n",
            "news # 4\n",
            "  the text is  : In addition to calls from Toscafund to oust its Executive Chairman, Jan Astrand, Speedy Hire has also rejected calls for a merger with HSS Hire.\r\n",
            "\n",
            "-------------\n",
            "the summary is : Speedy Hire rejects calls for merger with HSS\n",
            "\n",
            "news # 5\n",
            "  the text is  : TransferWise has announced plans to launch a currency transfer service to China this year. However to begin with, the new service won’t allow transfers out of the country, it will only convert other currencies into yuan, buying the currency on the wholesale market. “Our customers will be able to transfer their money from China over TransferWise by the end of the year,” the company said in a statement.\r\n",
            "\n",
            "-------------\n",
            "the summary is : TransferWise to launch China services\r\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4iCevEI6mkP"
      },
      "source": [
        "## Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgki8Rny6mkP"
      },
      "source": [
        "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXEYjxGE6mkT"
      },
      "source": [
        "def clean_text(text, remove_stopwords = True):\n",
        "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
        "    \n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Replace contractions with their longer forms \n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    \n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    \n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbYOStM-6mkV"
      },
      "source": [
        "We will remove the stopwords from the texts because they do not provide much use for training our model. However, we will keep them for our summaries so that they sound more like natural phrases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da1ofCGn6mkW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ed0f5fbb-50b0-47dc-c653-3257f9e95302"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "  \n",
        "# Clean the summaries and texts\n",
        "clean_summaries = []\n",
        "for summary in news.Summary:\n",
        "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
        "print(\"Summaries are complete.\")\n",
        "\n",
        "clean_texts = []\n",
        "for text in news.Text:\n",
        "    clean_texts.append(clean_text(text))\n",
        "print(\"Texts are complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Summaries are complete.\n",
            "Texts are complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq7WN7ru6mka",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "206f6c34-cf13-4935-dd6f-70db4c54b098"
      },
      "source": [
        "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
        "for i in range(5):\n",
        "    print(\"Clean news #\",i+1)\n",
        "    print(\"text  : \",clean_texts[i])\n",
        "    print(\"summry: \",clean_summaries[i])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clean news # 1\n",
            "text  :  animals must stunned prior killed order halal kosher meat marketed official european union eu organic label european court justice ruled judges argued organic labelling reflects highest animal welfare standards stunning integral significantly reduces suffering uk’s food standards agency estimates 88 animals killed halal methods stunned however minority halal – kosher – meat produced without stunning case brought french animal welfare association 2012\n",
            "summry:  non stunned halal and kosher meat not saleable as organic  ecj\n",
            "\n",
            "Clean news # 2\n",
            "text  :  researchers canada developed innovative solar cell uses bacteria convert light energy team university british columbia developed way genetically engineering e coli produce large amounts lycopene natural dye bacteria use photosynthesis coating bacteria semiconducting substance incorporating battery cell able achieve current density 0 686 milliamps per square centimetre say highest yet achieved biogenic solar cell\n",
            "summry:  solar cells powered by hybrid e  coli convert light to energy\n",
            "\n",
            "Clean news # 3\n",
            "text  :  southeast asia unprepared rapidly rising threat extremist attacks warns think tank institute policy analysis conflict ipac suggests main danger southern philippines islamic extremist groups sworn allegiance isis groups links parts region particularly malaysia indonesia isis also endorsed philippines based militant leader southeast asia widened extremist recruitment pool region opening channels international funding communication ipac says isis loses territory syria iraq raises risk revenge attacks southeast asia\n",
            "summry:  southeast asia unprepared for isis attacks says think tank\n",
            "\n",
            "Clean news # 4\n",
            "text  :  addition calls toscafund oust executive chairman jan astrand speedy hire also rejected calls merger hss hire\n",
            "summry:  speedy hire rejects calls for merger with hss\n",
            "\n",
            "Clean news # 5\n",
            "text  :  transferwise announced plans launch currency transfer service china year however begin new service won’t allow transfers country convert currencies yuan buying currency wholesale market “our customers able transfer money china transferwise end year ” company said statement\n",
            "summry:  transferwise to launch china services\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov6gNQzH6mkd"
      },
      "source": [
        "def count_words(count_dict, text):\n",
        "    '''Count the number of occurrences of each word in a set of text'''\n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL2TwE406mkh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "258875ad-43ec-4392-af24-e9a4b6b69af4"
      },
      "source": [
        "# Find the number of times each word was used and the size of the vocabulary\n",
        "word_counts = {}\n",
        "\n",
        "count_words(word_counts, clean_summaries)\n",
        "count_words(word_counts, clean_texts)\n",
        "            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 80991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paiPI3pR6mko",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2cc61272-bb5e-42a1-9f8e-e21388f9e014"
      },
      "source": [
        "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
        "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/numberbatch-en.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 516783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQgzpE_q6mkr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "17f34b69-d1ca-4de1-a0c0-819fa1476338"
      },
      "source": [
        "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
        "missing_words = 0\n",
        "threshold = 20\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
        "            \n",
        "print(\"Number of words missing from CN:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words missing from CN: 948\n",
            "Percent of words that are missing from vocabulary: 1.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4m_anQh6mkx"
      },
      "source": [
        "I use a threshold of 20, so that words not in CN can be added to our \n",
        "\n",
        "1.   List item\n",
        "\n",
        "1.   List item\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "2.   List item\n",
        "\n",
        "\n",
        "2.   List item\n",
        "\n",
        "word_embedding_matrix, but they need to be common enough in the reviews so that the model can understand their meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wR-Uv4F6mky",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e3c27b32-2f37-4a9e-cf82-8bd2abfa0e9f"
      },
      "source": [
        "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
        "\n",
        "#dictionary to convert words to integers\n",
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique words: 80991\n",
            "Number of words we will use: 42699\n",
            "Percent of words we will use: 52.72%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sPlq0P06mk4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7115e6ec-af51-48b8-af9f-1ff8be3ccb87"
      },
      "source": [
        "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in CN, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCQ36Smn6mk-"
      },
      "source": [
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    '''Convert words in text to an integer.\n",
        "       If word is not in vocab_to_int, use UNK's integer.\n",
        "       Total the number of words and UNKs.\n",
        "       Add EOS token to the end of texts'''\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "547FFl3u6mlA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e6834b0d-683b-43ca-caad-e287e5494d17"
      },
      "source": [
        "# Apply convert_to_ints to clean_summaries and clean_texts\n",
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in headlines: 2512789\n",
            "Total number of UNKs in headlines: 85158\n",
            "Percent of words that are UNK: 3.39%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnRzSx6M6mlD"
      },
      "source": [
        "def create_lengths(text):\n",
        "    '''Create a data frame of the sentence lengths from a text'''\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_i-9rYV6mlF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b9d43460-e9d3-4609-c2bc-01f3ea1477b3"
      },
      "source": [
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "print(\"Summaries:\")\n",
        "print(lengths_summaries.describe())\n",
        "print()\n",
        "print(\"Texts:\")\n",
        "print(lengths_texts.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries:\n",
            "             counts\n",
            "count  39986.000000\n",
            "mean       9.174261\n",
            "std        1.975717\n",
            "min        1.000000\n",
            "25%        8.000000\n",
            "50%        9.000000\n",
            "75%       10.000000\n",
            "max       35.000000\n",
            "\n",
            "Texts:\n",
            "             counts\n",
            "count  39986.000000\n",
            "mean      54.667459\n",
            "std       17.393107\n",
            "min        1.000000\n",
            "25%       45.000000\n",
            "50%       55.000000\n",
            "75%       63.000000\n",
            "max      537.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVMlHmV46mlI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d183442c-33cb-4299-ec5e-77b4f8598c18"
      },
      "source": [
        "# Inspect the length of texts\n",
        "print(np.percentile(lengths_texts.counts, 90))\n",
        "print(np.percentile(lengths_texts.counts, 95))\n",
        "print(np.percentile(lengths_texts.counts, 99))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70.0\n",
            "77.0\n",
            "103.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic8buaf16mlL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b5d0d1b7-5aa6-4c83-871f-20b38b0440a8"
      },
      "source": [
        "# Inspect the length of summaries\n",
        "print(np.percentile(lengths_summaries.counts, 90))\n",
        "print(np.percentile(lengths_summaries.counts, 95))\n",
        "print(np.percentile(lengths_summaries.counts, 99))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11.0\n",
            "12.0\n",
            "14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPW-kF136mlQ"
      },
      "source": [
        "def unk_counter(sentence):\n",
        "    '''Counts the number of time UNK appears in a sentence.'''\n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbFt7RTA6mlU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "99d01b6a-376e-4591-af2e-21f393d5c26c"
      },
      "source": [
        "# takes a long time  , this is normal\n",
        "\n",
        "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
        "# Limit the length of summaries and texts based on the min and max ranges.\n",
        "# Remove reviews that include too many UNKs\n",
        "\n",
        "sorted_summaries = []\n",
        "sorted_texts = []\n",
        "max_text_length = 70\n",
        "max_summary_length = 14\n",
        "min_length = 2\n",
        "unk_text_limit = 1\n",
        "unk_summary_limit = 0\n",
        "\n",
        "for length in range(min(lengths_texts.counts), max_text_length): \n",
        "    for count, words in enumerate(int_summaries):\n",
        "        if (len(int_summaries[count]) >= min_length and\n",
        "            len(int_summaries[count]) <= max_summary_length and\n",
        "            len(int_texts[count]) >= min_length and\n",
        "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
        "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
        "            length == len(int_texts[count])\n",
        "           ):\n",
        "            sorted_summaries.append(int_summaries[count])\n",
        "            sorted_texts.append(int_texts[count])\n",
        "        \n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17278\n",
            "17278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYnQFLER6mlY"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1MIZPcl6mlZ"
      },
      "source": [
        "def model_inputs():\n",
        "    '''Create palceholders for inputs to the model'''\n",
        "    \n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGoxdoHz6mlc"
      },
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
        "    \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ7p3phH6mld"
      },
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    '''Create the encoding layer'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "    # Join outputs since we are using a bidirectional RNN\n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejCY-IG56mlg"
      },
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "    '''Create the training logits'''\n",
        "    \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5RpDsV56mll"
      },
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBcDmTXx6mln"
      },
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "            \n",
        "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
        "    #                                                                _zero_state_tensors(rnn_size, \n",
        "    #                                                                                    batch_size, \n",
        "    #                                                                                    tf.float32)) \n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_decoder = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "        \n",
        "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_decoder = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        \n",
        "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuVESpih6mlo"
      },
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6w_YGts6mlq"
      },
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW6wc5VA6mls"
      },
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUhPBryV6mlv"
      },
      "source": [
        "# Set the Hyperparameters\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "rnn_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.005\n",
        "keep_probability = 0.75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I3bJqTD6mlx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "66998e86-58a8-4f65-8e83-a3928ff87f75"
      },
      "source": [
        "\n",
        "# Build the graph\n",
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "    \n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-33-9390cb26606a>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-33-9390cb26606a>:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Graph is built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmFOlquE6ml0"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xprfbevu6ml2"
      },
      "source": [
        "Since I am training this model on my MacBook Pro, it would take me days if I used the whole dataset. For this reason, I am only going to use a subset of the data, so that I can train it over night. Normally I use [FloydHub's](https://www.floydhub.com/) services for my GPU needs, but it would take quite a bit of time to upload the dataset and ConceptNet Numberbatch, so I'm not going to bother with that for this project.\n",
        "\n",
        "I chose not use use the start of the subset because I didn't want to make it too easy for my model. The texts that I am using are closer to the median lengths; I thought this would be more fair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlNcRq4j6ml3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ad99a3e3-7724-4dd2-91cd-f781cd3bce08"
      },
      "source": [
        "# Subset the data for training\n",
        "start = 100\n",
        "end = start + 100000\n",
        "sorted_summaries_short = sorted_summaries[start:end]\n",
        "sorted_texts_short = sorted_texts[start:end]\n",
        "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
        "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shortest text length: 16\n",
            "The longest text length: 69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XASfketR6ml6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db6df17b-7a70-44a8-93fc-25a7622eda07"
      },
      "source": [
        "# Train the Model\n",
        "learning_rate_decay = 0.95\n",
        "min_learning_rate = 0.0005\n",
        "display_step = 20 # Check training loss after every 20 batches\n",
        "stop_early = 0 \n",
        "stop = 6 #3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
        "per_epoch = 3 # Make 3 update checks per epoch\n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
        "\n",
        "  \n",
        "tf.reset_default_graph()\n",
        "checkpoint = \"/content/drive/Model 1/Model 300/best_model.ckpt\"  #300k sentence\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # If we want to continue training a previous session\n",
        "    # loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    # loader.restore(sess, checkpoint)\n",
        "    #sess.run(tf.local_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts_short) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "                \n",
        "                #saver = tf.train.Saver() \n",
        "                #saver.save(sess, checkpoint)\n",
        "                \n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "              \n",
        "                  \n",
        "                # If the update loss is at a new minimum, save the model\n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "                    \n",
        "        # Reduce learning rate, but not below its minimum value\n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   1/100 Batch   20/268 - Loss:  7.356, Seconds: 3.12\n",
            "Epoch   1/100 Batch   40/268 - Loss:  5.572, Seconds: 3.13\n",
            "Epoch   1/100 Batch   60/268 - Loss:  5.526, Seconds: 3.07\n",
            "Epoch   1/100 Batch   80/268 - Loss:  5.412, Seconds: 3.76\n",
            "Average loss for this update: 5.92\n",
            "New Record!\n",
            "Epoch   1/100 Batch  100/268 - Loss:  5.484, Seconds: 3.63\n",
            "Epoch   1/100 Batch  120/268 - Loss:  5.563, Seconds: 3.86\n",
            "Epoch   1/100 Batch  140/268 - Loss:  5.367, Seconds: 3.59\n",
            "Epoch   1/100 Batch  160/268 - Loss:  5.550, Seconds: 3.77\n",
            "Average loss for this update: 5.467\n",
            "New Record!\n",
            "Epoch   1/100 Batch  180/268 - Loss:  5.317, Seconds: 3.99\n",
            "Epoch   1/100 Batch  200/268 - Loss:  5.165, Seconds: 4.38\n",
            "Epoch   1/100 Batch  220/268 - Loss:  5.191, Seconds: 4.22\n",
            "Epoch   1/100 Batch  240/268 - Loss:  5.293, Seconds: 4.45\n",
            "Epoch   1/100 Batch  260/268 - Loss:  5.115, Seconds: 4.45\n",
            "Average loss for this update: 5.176\n",
            "New Record!\n",
            "Epoch   2/100 Batch   20/268 - Loss:  4.826, Seconds: 3.03\n",
            "Epoch   2/100 Batch   40/268 - Loss:  4.563, Seconds: 3.40\n",
            "Epoch   2/100 Batch   60/268 - Loss:  4.583, Seconds: 3.09\n",
            "Epoch   2/100 Batch   80/268 - Loss:  4.494, Seconds: 3.90\n",
            "Average loss for this update: 4.612\n",
            "New Record!\n",
            "Epoch   2/100 Batch  100/268 - Loss:  4.613, Seconds: 4.06\n",
            "Epoch   2/100 Batch  120/268 - Loss:  4.693, Seconds: 3.87\n",
            "Epoch   2/100 Batch  140/268 - Loss:  4.562, Seconds: 3.67\n",
            "Epoch   2/100 Batch  160/268 - Loss:  4.730, Seconds: 3.73\n",
            "Average loss for this update: 4.643\n",
            "No Improvement.\n",
            "Epoch   2/100 Batch  180/268 - Loss:  4.550, Seconds: 3.92\n",
            "Epoch   2/100 Batch  200/268 - Loss:  4.451, Seconds: 4.10\n",
            "Epoch   2/100 Batch  220/268 - Loss:  4.493, Seconds: 4.14\n",
            "Epoch   2/100 Batch  240/268 - Loss:  4.572, Seconds: 4.03\n",
            "Epoch   2/100 Batch  260/268 - Loss:  4.437, Seconds: 4.33\n",
            "Average loss for this update: 4.475\n",
            "New Record!\n",
            "Epoch   3/100 Batch   20/268 - Loss:  4.257, Seconds: 2.99\n",
            "Epoch   3/100 Batch   40/268 - Loss:  4.042, Seconds: 3.06\n",
            "Epoch   3/100 Batch   60/268 - Loss:  4.030, Seconds: 3.47\n",
            "Epoch   3/100 Batch   80/268 - Loss:  3.929, Seconds: 3.83\n",
            "Average loss for this update: 4.058\n",
            "New Record!\n",
            "Epoch   3/100 Batch  100/268 - Loss:  4.052, Seconds: 3.70\n",
            "Epoch   3/100 Batch  120/268 - Loss:  4.132, Seconds: 3.88\n",
            "Epoch   3/100 Batch  140/268 - Loss:  4.041, Seconds: 3.92\n",
            "Epoch   3/100 Batch  160/268 - Loss:  4.169, Seconds: 3.55\n",
            "Average loss for this update: 4.099\n",
            "No Improvement.\n",
            "Epoch   3/100 Batch  180/268 - Loss:  4.027, Seconds: 4.01\n",
            "Epoch   3/100 Batch  200/268 - Loss:  3.938, Seconds: 4.07\n",
            "Epoch   3/100 Batch  220/268 - Loss:  3.989, Seconds: 4.35\n",
            "Epoch   3/100 Batch  240/268 - Loss:  4.036, Seconds: 4.22\n",
            "Epoch   3/100 Batch  260/268 - Loss:  3.947, Seconds: 4.36\n",
            "Average loss for this update: 3.965\n",
            "New Record!\n",
            "Epoch   4/100 Batch   20/268 - Loss:  3.852, Seconds: 2.98\n",
            "Epoch   4/100 Batch   40/268 - Loss:  3.611, Seconds: 3.36\n",
            "Epoch   4/100 Batch   60/268 - Loss:  3.635, Seconds: 3.10\n",
            "Epoch   4/100 Batch   80/268 - Loss:  3.554, Seconds: 3.47\n",
            "Average loss for this update: 3.659\n",
            "New Record!\n",
            "Epoch   4/100 Batch  100/268 - Loss:  3.688, Seconds: 3.62\n",
            "Epoch   4/100 Batch  120/268 - Loss:  3.734, Seconds: 3.69\n",
            "Epoch   4/100 Batch  140/268 - Loss:  3.678, Seconds: 3.73\n",
            "Epoch   4/100 Batch  160/268 - Loss:  3.791, Seconds: 3.76\n",
            "Average loss for this update: 3.727\n",
            "No Improvement.\n",
            "Epoch   4/100 Batch  180/268 - Loss:  3.677, Seconds: 4.04\n",
            "Epoch   4/100 Batch  200/268 - Loss:  3.618, Seconds: 4.00\n",
            "Epoch   4/100 Batch  220/268 - Loss:  3.664, Seconds: 4.28\n",
            "Epoch   4/100 Batch  240/268 - Loss:  3.701, Seconds: 4.00\n",
            "Epoch   4/100 Batch  260/268 - Loss:  3.622, Seconds: 4.90\n",
            "Average loss for this update: 3.641\n",
            "New Record!\n",
            "Epoch   5/100 Batch   20/268 - Loss:  3.517, Seconds: 2.98\n",
            "Epoch   5/100 Batch   40/268 - Loss:  3.334, Seconds: 3.39\n",
            "Epoch   5/100 Batch   60/268 - Loss:  3.342, Seconds: 3.43\n",
            "Epoch   5/100 Batch   80/268 - Loss:  3.262, Seconds: 3.91\n",
            "Average loss for this update: 3.36\n",
            "New Record!\n",
            "Epoch   5/100 Batch  100/268 - Loss:  3.377, Seconds: 3.65\n",
            "Epoch   5/100 Batch  120/268 - Loss:  3.455, Seconds: 3.68\n",
            "Epoch   5/100 Batch  140/268 - Loss:  3.388, Seconds: 3.87\n",
            "Epoch   5/100 Batch  160/268 - Loss:  3.480, Seconds: 3.57\n",
            "Average loss for this update: 3.432\n",
            "No Improvement.\n",
            "Epoch   5/100 Batch  180/268 - Loss:  3.407, Seconds: 4.22\n",
            "Epoch   5/100 Batch  200/268 - Loss:  3.337, Seconds: 3.91\n",
            "Epoch   5/100 Batch  220/268 - Loss:  3.374, Seconds: 4.71\n",
            "Epoch   5/100 Batch  240/268 - Loss:  3.425, Seconds: 4.01\n",
            "Epoch   5/100 Batch  260/268 - Loss:  3.371, Seconds: 5.07\n",
            "Average loss for this update: 3.372\n",
            "No Improvement.\n",
            "Epoch   6/100 Batch   20/268 - Loss:  3.230, Seconds: 2.99\n",
            "Epoch   6/100 Batch   40/268 - Loss:  3.093, Seconds: 3.20\n",
            "Epoch   6/100 Batch   60/268 - Loss:  3.108, Seconds: 3.12\n",
            "Epoch   6/100 Batch   80/268 - Loss:  3.012, Seconds: 3.57\n",
            "Average loss for this update: 3.107\n",
            "New Record!\n",
            "Epoch   6/100 Batch  100/268 - Loss:  3.123, Seconds: 3.70\n",
            "Epoch   6/100 Batch  120/268 - Loss:  3.206, Seconds: 3.68\n",
            "Epoch   6/100 Batch  140/268 - Loss:  3.139, Seconds: 3.79\n",
            "Epoch   6/100 Batch  160/268 - Loss:  3.213, Seconds: 3.58\n",
            "Average loss for this update: 3.18\n",
            "No Improvement.\n",
            "Epoch   6/100 Batch  180/268 - Loss:  3.163, Seconds: 3.96\n",
            "Epoch   6/100 Batch  200/268 - Loss:  3.106, Seconds: 4.07\n",
            "Epoch   6/100 Batch  220/268 - Loss:  3.150, Seconds: 4.32\n",
            "Epoch   6/100 Batch  240/268 - Loss:  3.179, Seconds: 4.68\n",
            "Epoch   6/100 Batch  260/268 - Loss:  3.134, Seconds: 4.60\n",
            "Average loss for this update: 3.135\n",
            "No Improvement.\n",
            "Epoch   7/100 Batch   20/268 - Loss:  2.980, Seconds: 3.23\n",
            "Epoch   7/100 Batch   40/268 - Loss:  2.866, Seconds: 3.06\n",
            "Epoch   7/100 Batch   60/268 - Loss:  2.867, Seconds: 3.21\n",
            "Epoch   7/100 Batch   80/268 - Loss:  2.795, Seconds: 3.83\n",
            "Average loss for this update: 2.876\n",
            "New Record!\n",
            "Epoch   7/100 Batch  100/268 - Loss:  2.919, Seconds: 3.58\n",
            "Epoch   7/100 Batch  120/268 - Loss:  2.976, Seconds: 3.80\n",
            "Epoch   7/100 Batch  140/268 - Loss:  2.920, Seconds: 3.64\n",
            "Epoch   7/100 Batch  160/268 - Loss:  3.019, Seconds: 4.40\n",
            "Average loss for this update: 2.969\n",
            "No Improvement.\n",
            "Epoch   7/100 Batch  180/268 - Loss:  2.956, Seconds: 3.99\n",
            "Epoch   7/100 Batch  200/268 - Loss:  2.905, Seconds: 4.26\n",
            "Epoch   7/100 Batch  220/268 - Loss:  2.948, Seconds: 4.14\n",
            "Epoch   7/100 Batch  240/268 - Loss:  2.978, Seconds: 4.08\n",
            "Epoch   7/100 Batch  260/268 - Loss:  2.954, Seconds: 4.96\n",
            "Average loss for this update: 2.938\n",
            "No Improvement.\n",
            "Epoch   8/100 Batch   20/268 - Loss:  2.755, Seconds: 2.97\n",
            "Epoch   8/100 Batch   40/268 - Loss:  2.646, Seconds: 3.05\n",
            "Epoch   8/100 Batch   60/268 - Loss:  2.700, Seconds: 3.43\n",
            "Epoch   8/100 Batch   80/268 - Loss:  2.613, Seconds: 3.56\n",
            "Average loss for this update: 2.677\n",
            "New Record!\n",
            "Epoch   8/100 Batch  100/268 - Loss:  2.718, Seconds: 3.62\n",
            "Epoch   8/100 Batch  120/268 - Loss:  2.805, Seconds: 3.81\n",
            "Epoch   8/100 Batch  140/268 - Loss:  2.734, Seconds: 3.79\n",
            "Epoch   8/100 Batch  160/268 - Loss:  2.820, Seconds: 4.02\n",
            "Average loss for this update: 2.784\n",
            "No Improvement.\n",
            "Epoch   8/100 Batch  180/268 - Loss:  2.782, Seconds: 4.01\n",
            "Epoch   8/100 Batch  200/268 - Loss:  2.730, Seconds: 4.02\n",
            "Epoch   8/100 Batch  220/268 - Loss:  2.764, Seconds: 4.15\n",
            "Epoch   8/100 Batch  240/268 - Loss:  2.784, Seconds: 4.04\n",
            "Epoch   8/100 Batch  260/268 - Loss:  2.787, Seconds: 4.42\n",
            "Average loss for this update: 2.759\n",
            "No Improvement.\n",
            "Epoch   9/100 Batch   20/268 - Loss:  2.603, Seconds: 3.01\n",
            "Epoch   9/100 Batch   40/268 - Loss:  2.465, Seconds: 3.41\n",
            "Epoch   9/100 Batch   60/268 - Loss:  2.551, Seconds: 3.27\n",
            "Epoch   9/100 Batch   80/268 - Loss:  2.455, Seconds: 3.89\n",
            "Average loss for this update: 2.519\n",
            "New Record!\n",
            "Epoch   9/100 Batch  100/268 - Loss:  2.564, Seconds: 4.07\n",
            "Epoch   9/100 Batch  120/268 - Loss:  2.647, Seconds: 3.67\n",
            "Epoch   9/100 Batch  140/268 - Loss:  2.596, Seconds: 3.81\n",
            "Epoch   9/100 Batch  160/268 - Loss:  2.679, Seconds: 3.79\n",
            "Average loss for this update: 2.633\n",
            "No Improvement.\n",
            "Epoch   9/100 Batch  180/268 - Loss:  2.633, Seconds: 4.35\n",
            "Epoch   9/100 Batch  200/268 - Loss:  2.594, Seconds: 4.07\n",
            "Epoch   9/100 Batch  220/268 - Loss:  2.603, Seconds: 4.06\n",
            "Epoch   9/100 Batch  240/268 - Loss:  2.623, Seconds: 4.20\n",
            "Epoch   9/100 Batch  260/268 - Loss:  2.620, Seconds: 4.36\n",
            "Average loss for this update: 2.604\n",
            "No Improvement.\n",
            "Epoch  10/100 Batch   20/268 - Loss:  2.463, Seconds: 3.20\n",
            "Epoch  10/100 Batch   40/268 - Loss:  2.326, Seconds: 3.43\n",
            "Epoch  10/100 Batch   60/268 - Loss:  2.421, Seconds: 3.23\n",
            "Epoch  10/100 Batch   80/268 - Loss:  2.325, Seconds: 3.52\n",
            "Average loss for this update: 2.385\n",
            "New Record!\n",
            "Epoch  10/100 Batch  100/268 - Loss:  2.442, Seconds: 3.72\n",
            "Epoch  10/100 Batch  120/268 - Loss:  2.484, Seconds: 3.68\n",
            "Epoch  10/100 Batch  140/268 - Loss:  2.439, Seconds: 3.63\n",
            "Epoch  10/100 Batch  160/268 - Loss:  2.508, Seconds: 3.71\n",
            "Average loss for this update: 2.48\n",
            "No Improvement.\n",
            "Epoch  10/100 Batch  180/268 - Loss:  2.500, Seconds: 4.02\n",
            "Epoch  10/100 Batch  200/268 - Loss:  2.440, Seconds: 4.06\n",
            "Epoch  10/100 Batch  220/268 - Loss:  2.488, Seconds: 4.37\n",
            "Epoch  10/100 Batch  240/268 - Loss:  2.489, Seconds: 4.04\n",
            "Epoch  10/100 Batch  260/268 - Loss:  2.496, Seconds: 4.40\n",
            "Average loss for this update: 2.475\n",
            "No Improvement.\n",
            "Epoch  11/100 Batch   20/268 - Loss:  2.354, Seconds: 3.43\n",
            "Epoch  11/100 Batch   40/268 - Loss:  2.245, Seconds: 3.17\n",
            "Epoch  11/100 Batch   60/268 - Loss:  2.293, Seconds: 3.08\n",
            "Epoch  11/100 Batch   80/268 - Loss:  2.225, Seconds: 3.56\n",
            "Average loss for this update: 2.278\n",
            "New Record!\n",
            "Epoch  11/100 Batch  100/268 - Loss:  2.303, Seconds: 3.65\n",
            "Epoch  11/100 Batch  120/268 - Loss:  2.383, Seconds: 4.07\n",
            "Epoch  11/100 Batch  140/268 - Loss:  2.318, Seconds: 3.54\n",
            "Epoch  11/100 Batch  160/268 - Loss:  2.388, Seconds: 3.68\n",
            "Average loss for this update: 2.357\n",
            "No Improvement.\n",
            "Epoch  11/100 Batch  180/268 - Loss:  2.358, Seconds: 4.21\n",
            "Epoch  11/100 Batch  200/268 - Loss:  2.351, Seconds: 4.15\n",
            "Epoch  11/100 Batch  220/268 - Loss:  2.339, Seconds: 4.35\n",
            "Epoch  11/100 Batch  240/268 - Loss:  2.359, Seconds: 4.76\n",
            "Epoch  11/100 Batch  260/268 - Loss:  2.380, Seconds: 5.05\n",
            "Average loss for this update: 2.353\n",
            "No Improvement.\n",
            "Epoch  12/100 Batch   20/268 - Loss:  2.214, Seconds: 3.11\n",
            "Epoch  12/100 Batch   40/268 - Loss:  2.126, Seconds: 3.08\n",
            "Epoch  12/100 Batch   60/268 - Loss:  2.170, Seconds: 3.42\n",
            "Epoch  12/100 Batch   80/268 - Loss:  2.113, Seconds: 3.50\n",
            "Average loss for this update: 2.156\n",
            "New Record!\n",
            "Epoch  12/100 Batch  100/268 - Loss:  2.191, Seconds: 3.75\n",
            "Epoch  12/100 Batch  120/268 - Loss:  2.257, Seconds: 3.72\n",
            "Epoch  12/100 Batch  140/268 - Loss:  2.212, Seconds: 4.10\n",
            "Epoch  12/100 Batch  160/268 - Loss:  2.278, Seconds: 3.74\n",
            "Average loss for this update: 2.24\n",
            "No Improvement.\n",
            "Epoch  12/100 Batch  180/268 - Loss:  2.232, Seconds: 3.99\n",
            "Epoch  12/100 Batch  200/268 - Loss:  2.252, Seconds: 4.28\n",
            "Epoch  12/100 Batch  220/268 - Loss:  2.241, Seconds: 4.25\n",
            "Epoch  12/100 Batch  240/268 - Loss:  2.254, Seconds: 4.47\n",
            "Epoch  12/100 Batch  260/268 - Loss:  2.255, Seconds: 4.34\n",
            "Average loss for this update: 2.248\n",
            "No Improvement.\n",
            "Epoch  13/100 Batch   20/268 - Loss:  2.091, Seconds: 3.01\n",
            "Epoch  13/100 Batch   40/268 - Loss:  2.039, Seconds: 3.18\n",
            "Epoch  13/100 Batch   60/268 - Loss:  2.080, Seconds: 3.13\n",
            "Epoch  13/100 Batch   80/268 - Loss:  2.013, Seconds: 3.60\n",
            "Average loss for this update: 2.055\n",
            "New Record!\n",
            "Epoch  13/100 Batch  100/268 - Loss:  2.090, Seconds: 3.61\n",
            "Epoch  13/100 Batch  120/268 - Loss:  2.150, Seconds: 4.06\n",
            "Epoch  13/100 Batch  140/268 - Loss:  2.119, Seconds: 3.67\n",
            "Epoch  13/100 Batch  160/268 - Loss:  2.171, Seconds: 3.95\n",
            "Average loss for this update: 2.142\n",
            "No Improvement.\n",
            "Epoch  13/100 Batch  180/268 - Loss:  2.150, Seconds: 3.95\n",
            "Epoch  13/100 Batch  200/268 - Loss:  2.140, Seconds: 3.95\n",
            "Epoch  13/100 Batch  220/268 - Loss:  2.136, Seconds: 4.15\n",
            "Epoch  13/100 Batch  240/268 - Loss:  2.144, Seconds: 4.17\n",
            "Epoch  13/100 Batch  260/268 - Loss:  2.149, Seconds: 4.56\n",
            "Average loss for this update: 2.14\n",
            "No Improvement.\n",
            "Epoch  14/100 Batch   20/268 - Loss:  1.972, Seconds: 3.05\n",
            "Epoch  14/100 Batch   40/268 - Loss:  1.925, Seconds: 3.27\n",
            "Epoch  14/100 Batch   60/268 - Loss:  2.004, Seconds: 3.16\n",
            "Epoch  14/100 Batch   80/268 - Loss:  1.937, Seconds: 3.53\n",
            "Average loss for this update: 1.958\n",
            "New Record!\n",
            "Epoch  14/100 Batch  100/268 - Loss:  1.988, Seconds: 3.97\n",
            "Epoch  14/100 Batch  120/268 - Loss:  2.052, Seconds: 4.00\n",
            "Epoch  14/100 Batch  140/268 - Loss:  2.041, Seconds: 4.08\n",
            "Epoch  14/100 Batch  160/268 - Loss:  2.068, Seconds: 4.09\n",
            "Average loss for this update: 2.046\n",
            "No Improvement.\n",
            "Epoch  14/100 Batch  180/268 - Loss:  2.044, Seconds: 4.49\n",
            "Epoch  14/100 Batch  200/268 - Loss:  2.047, Seconds: 4.02\n",
            "Epoch  14/100 Batch  220/268 - Loss:  2.055, Seconds: 4.05\n",
            "Epoch  14/100 Batch  240/268 - Loss:  2.025, Seconds: 4.27\n",
            "Epoch  14/100 Batch  260/268 - Loss:  2.036, Seconds: 5.33\n",
            "Average loss for this update: 2.038\n",
            "No Improvement.\n",
            "Epoch  15/100 Batch   20/268 - Loss:  1.898, Seconds: 3.02\n",
            "Epoch  15/100 Batch   40/268 - Loss:  1.832, Seconds: 3.10\n",
            "Epoch  15/100 Batch   60/268 - Loss:  1.921, Seconds: 3.22\n",
            "Epoch  15/100 Batch   80/268 - Loss:  1.850, Seconds: 3.52\n",
            "Average loss for this update: 1.875\n",
            "New Record!\n",
            "Epoch  15/100 Batch  100/268 - Loss:  1.910, Seconds: 4.01\n",
            "Epoch  15/100 Batch  120/268 - Loss:  1.964, Seconds: 4.07\n",
            "Epoch  15/100 Batch  140/268 - Loss:  1.939, Seconds: 3.64\n",
            "Epoch  15/100 Batch  160/268 - Loss:  1.961, Seconds: 3.62\n",
            "Average loss for this update: 1.95\n",
            "No Improvement.\n",
            "Epoch  15/100 Batch  180/268 - Loss:  1.956, Seconds: 4.06\n",
            "Epoch  15/100 Batch  200/268 - Loss:  1.961, Seconds: 4.20\n",
            "Epoch  15/100 Batch  220/268 - Loss:  1.972, Seconds: 4.10\n",
            "Epoch  15/100 Batch  240/268 - Loss:  1.953, Seconds: 4.68\n",
            "Epoch  15/100 Batch  260/268 - Loss:  1.959, Seconds: 4.29\n",
            "Average loss for this update: 1.96\n",
            "No Improvement.\n",
            "Epoch  16/100 Batch   20/268 - Loss:  1.809, Seconds: 3.15\n",
            "Epoch  16/100 Batch   40/268 - Loss:  1.752, Seconds: 3.32\n",
            "Epoch  16/100 Batch   60/268 - Loss:  1.832, Seconds: 3.12\n",
            "Epoch  16/100 Batch   80/268 - Loss:  1.786, Seconds: 4.18\n",
            "Average loss for this update: 1.792\n",
            "New Record!\n",
            "Epoch  16/100 Batch  100/268 - Loss:  1.818, Seconds: 4.01\n",
            "Epoch  16/100 Batch  120/268 - Loss:  1.890, Seconds: 3.93\n",
            "Epoch  16/100 Batch  140/268 - Loss:  1.844, Seconds: 3.65\n",
            "Epoch  16/100 Batch  160/268 - Loss:  1.889, Seconds: 4.07\n",
            "Average loss for this update: 1.87\n",
            "No Improvement.\n",
            "Epoch  16/100 Batch  180/268 - Loss:  1.869, Seconds: 4.66\n",
            "Epoch  16/100 Batch  200/268 - Loss:  1.902, Seconds: 3.98\n",
            "Epoch  16/100 Batch  220/268 - Loss:  1.891, Seconds: 4.35\n",
            "Epoch  16/100 Batch  240/268 - Loss:  1.855, Seconds: 4.26\n",
            "Epoch  16/100 Batch  260/268 - Loss:  1.883, Seconds: 4.38\n",
            "Average loss for this update: 1.879\n",
            "No Improvement.\n",
            "Epoch  17/100 Batch   20/268 - Loss:  1.748, Seconds: 3.01\n",
            "Epoch  17/100 Batch   40/268 - Loss:  1.684, Seconds: 3.11\n",
            "Epoch  17/100 Batch   60/268 - Loss:  1.753, Seconds: 3.50\n",
            "Epoch  17/100 Batch   80/268 - Loss:  1.715, Seconds: 3.54\n",
            "Average loss for this update: 1.725\n",
            "New Record!\n",
            "Epoch  17/100 Batch  100/268 - Loss:  1.760, Seconds: 3.67\n",
            "Epoch  17/100 Batch  120/268 - Loss:  1.811, Seconds: 3.69\n",
            "Epoch  17/100 Batch  140/268 - Loss:  1.798, Seconds: 3.63\n",
            "Epoch  17/100 Batch  160/268 - Loss:  1.814, Seconds: 4.29\n",
            "Average loss for this update: 1.806\n",
            "No Improvement.\n",
            "Epoch  17/100 Batch  180/268 - Loss:  1.817, Seconds: 4.01\n",
            "Epoch  17/100 Batch  200/268 - Loss:  1.827, Seconds: 4.05\n",
            "Epoch  17/100 Batch  220/268 - Loss:  1.820, Seconds: 4.20\n",
            "Epoch  17/100 Batch  240/268 - Loss:  1.800, Seconds: 4.00\n",
            "Epoch  17/100 Batch  260/268 - Loss:  1.823, Seconds: 4.42\n",
            "Average loss for this update: 1.814\n",
            "No Improvement.\n",
            "Epoch  18/100 Batch   20/268 - Loss:  1.697, Seconds: 2.98\n",
            "Epoch  18/100 Batch   40/268 - Loss:  1.612, Seconds: 3.07\n",
            "Epoch  18/100 Batch   60/268 - Loss:  1.683, Seconds: 3.26\n",
            "Epoch  18/100 Batch   80/268 - Loss:  1.668, Seconds: 4.02\n",
            "Average loss for this update: 1.667\n",
            "New Record!\n",
            "Epoch  18/100 Batch  100/268 - Loss:  1.693, Seconds: 3.56\n",
            "Epoch  18/100 Batch  120/268 - Loss:  1.733, Seconds: 3.73\n",
            "Epoch  18/100 Batch  140/268 - Loss:  1.708, Seconds: 3.58\n",
            "Epoch  18/100 Batch  160/268 - Loss:  1.753, Seconds: 3.65\n",
            "Average loss for this update: 1.73\n",
            "No Improvement.\n",
            "Epoch  18/100 Batch  180/268 - Loss:  1.747, Seconds: 3.99\n",
            "Epoch  18/100 Batch  200/268 - Loss:  1.753, Seconds: 4.44\n",
            "Epoch  18/100 Batch  220/268 - Loss:  1.750, Seconds: 4.27\n",
            "Epoch  18/100 Batch  240/268 - Loss:  1.711, Seconds: 5.08\n",
            "Epoch  18/100 Batch  260/268 - Loss:  1.759, Seconds: 4.42\n",
            "Average loss for this update: 1.741\n",
            "No Improvement.\n",
            "Epoch  19/100 Batch   20/268 - Loss:  1.625, Seconds: 3.06\n",
            "Epoch  19/100 Batch   40/268 - Loss:  1.565, Seconds: 3.12\n",
            "Epoch  19/100 Batch   60/268 - Loss:  1.633, Seconds: 3.10\n",
            "Epoch  19/100 Batch   80/268 - Loss:  1.580, Seconds: 3.49\n",
            "Average loss for this update: 1.601\n",
            "New Record!\n",
            "Epoch  19/100 Batch  100/268 - Loss:  1.636, Seconds: 3.58\n",
            "Epoch  19/100 Batch  120/268 - Loss:  1.661, Seconds: 4.49\n",
            "Epoch  19/100 Batch  140/268 - Loss:  1.661, Seconds: 3.60\n",
            "Epoch  19/100 Batch  160/268 - Loss:  1.696, Seconds: 3.60\n",
            "Average loss for this update: 1.671\n",
            "No Improvement.\n",
            "Epoch  19/100 Batch  180/268 - Loss:  1.674, Seconds: 4.59\n",
            "Epoch  19/100 Batch  200/268 - Loss:  1.693, Seconds: 4.00\n",
            "Epoch  19/100 Batch  220/268 - Loss:  1.677, Seconds: 4.78\n",
            "Epoch  19/100 Batch  240/268 - Loss:  1.672, Seconds: 5.04\n",
            "Epoch  19/100 Batch  260/268 - Loss:  1.692, Seconds: 4.36\n",
            "Average loss for this update: 1.681\n",
            "No Improvement.\n",
            "Epoch  20/100 Batch   20/268 - Loss:  1.567, Seconds: 3.02\n",
            "Epoch  20/100 Batch   40/268 - Loss:  1.496, Seconds: 3.09\n",
            "Epoch  20/100 Batch   60/268 - Loss:  1.576, Seconds: 3.18\n",
            "Epoch  20/100 Batch   80/268 - Loss:  1.529, Seconds: 3.47\n",
            "Average loss for this update: 1.545\n",
            "New Record!\n",
            "Epoch  20/100 Batch  100/268 - Loss:  1.592, Seconds: 3.69\n",
            "Epoch  20/100 Batch  120/268 - Loss:  1.603, Seconds: 4.07\n",
            "Epoch  20/100 Batch  140/268 - Loss:  1.593, Seconds: 4.18\n",
            "Epoch  20/100 Batch  160/268 - Loss:  1.646, Seconds: 4.14\n",
            "Average loss for this update: 1.609\n",
            "No Improvement.\n",
            "Epoch  20/100 Batch  180/268 - Loss:  1.600, Seconds: 3.91\n",
            "Epoch  20/100 Batch  200/268 - Loss:  1.635, Seconds: 4.10\n",
            "Epoch  20/100 Batch  220/268 - Loss:  1.622, Seconds: 4.07\n",
            "Epoch  20/100 Batch  240/268 - Loss:  1.599, Seconds: 4.03\n",
            "Epoch  20/100 Batch  260/268 - Loss:  1.627, Seconds: 4.42\n",
            "Average loss for this update: 1.619\n",
            "No Improvement.\n",
            "Epoch  21/100 Batch   20/268 - Loss:  1.531, Seconds: 3.19\n",
            "Epoch  21/100 Batch   40/268 - Loss:  1.456, Seconds: 3.13\n",
            "Epoch  21/100 Batch   60/268 - Loss:  1.531, Seconds: 3.55\n",
            "Epoch  21/100 Batch   80/268 - Loss:  1.478, Seconds: 3.49\n",
            "Average loss for this update: 1.5\n",
            "New Record!\n",
            "Epoch  21/100 Batch  100/268 - Loss:  1.518, Seconds: 3.62\n",
            "Epoch  21/100 Batch  120/268 - Loss:  1.550, Seconds: 3.81\n",
            "Epoch  21/100 Batch  140/268 - Loss:  1.538, Seconds: 3.65\n",
            "Epoch  21/100 Batch  160/268 - Loss:  1.564, Seconds: 3.66\n",
            "Average loss for this update: 1.545\n",
            "No Improvement.\n",
            "Epoch  21/100 Batch  180/268 - Loss:  1.546, Seconds: 4.27\n",
            "Epoch  21/100 Batch  200/268 - Loss:  1.566, Seconds: 4.08\n",
            "Epoch  21/100 Batch  220/268 - Loss:  1.566, Seconds: 4.21\n",
            "Epoch  21/100 Batch  240/268 - Loss:  1.552, Seconds: 4.22\n",
            "Epoch  21/100 Batch  260/268 - Loss:  1.569, Seconds: 4.54\n",
            "Average loss for this update: 1.564\n",
            "No Improvement.\n",
            "Epoch  22/100 Batch   20/268 - Loss:  1.463, Seconds: 3.22\n",
            "Epoch  22/100 Batch   40/268 - Loss:  1.411, Seconds: 3.64\n",
            "Epoch  22/100 Batch   60/268 - Loss:  1.493, Seconds: 3.26\n",
            "Epoch  22/100 Batch   80/268 - Loss:  1.434, Seconds: 3.53\n",
            "Average loss for this update: 1.452\n",
            "New Record!\n",
            "Epoch  22/100 Batch  100/268 - Loss:  1.483, Seconds: 3.61\n",
            "Epoch  22/100 Batch  120/268 - Loss:  1.509, Seconds: 3.84\n",
            "Epoch  22/100 Batch  140/268 - Loss:  1.487, Seconds: 3.65\n",
            "Epoch  22/100 Batch  160/268 - Loss:  1.512, Seconds: 3.58\n",
            "Average loss for this update: 1.501\n",
            "No Improvement.\n",
            "Epoch  22/100 Batch  180/268 - Loss:  1.493, Seconds: 4.14\n",
            "Epoch  22/100 Batch  200/268 - Loss:  1.510, Seconds: 4.07\n",
            "Epoch  22/100 Batch  220/268 - Loss:  1.517, Seconds: 4.15\n",
            "Epoch  22/100 Batch  240/268 - Loss:  1.512, Seconds: 4.87\n",
            "Epoch  22/100 Batch  260/268 - Loss:  1.526, Seconds: 4.30\n",
            "Average loss for this update: 1.515\n",
            "No Improvement.\n",
            "Epoch  23/100 Batch   20/268 - Loss:  1.445, Seconds: 3.01\n",
            "Epoch  23/100 Batch   40/268 - Loss:  1.357, Seconds: 3.14\n",
            "Epoch  23/100 Batch   60/268 - Loss:  1.430, Seconds: 3.64\n",
            "Epoch  23/100 Batch   80/268 - Loss:  1.408, Seconds: 3.48\n",
            "Average loss for this update: 1.41\n",
            "New Record!\n",
            "Epoch  23/100 Batch  100/268 - Loss:  1.431, Seconds: 3.90\n",
            "Epoch  23/100 Batch  120/268 - Loss:  1.474, Seconds: 3.72\n",
            "Epoch  23/100 Batch  140/268 - Loss:  1.444, Seconds: 3.65\n",
            "Epoch  23/100 Batch  160/268 - Loss:  1.466, Seconds: 3.67\n",
            "Average loss for this update: 1.458\n",
            "No Improvement.\n",
            "Epoch  23/100 Batch  180/268 - Loss:  1.447, Seconds: 4.36\n",
            "Epoch  23/100 Batch  200/268 - Loss:  1.466, Seconds: 4.19\n",
            "Epoch  23/100 Batch  220/268 - Loss:  1.461, Seconds: 4.05\n",
            "Epoch  23/100 Batch  240/268 - Loss:  1.432, Seconds: 4.06\n",
            "Epoch  23/100 Batch  260/268 - Loss:  1.468, Seconds: 4.74\n",
            "Average loss for this update: 1.455\n",
            "No Improvement.\n",
            "Epoch  24/100 Batch   20/268 - Loss:  1.399, Seconds: 3.08\n",
            "Epoch  24/100 Batch   40/268 - Loss:  1.334, Seconds: 3.15\n",
            "Epoch  24/100 Batch   60/268 - Loss:  1.382, Seconds: 3.17\n",
            "Epoch  24/100 Batch   80/268 - Loss:  1.350, Seconds: 3.51\n",
            "Average loss for this update: 1.367\n",
            "New Record!\n",
            "Epoch  24/100 Batch  100/268 - Loss:  1.397, Seconds: 3.67\n",
            "Epoch  24/100 Batch  120/268 - Loss:  1.409, Seconds: 3.69\n",
            "Epoch  24/100 Batch  140/268 - Loss:  1.414, Seconds: 3.73\n",
            "Epoch  24/100 Batch  160/268 - Loss:  1.423, Seconds: 3.60\n",
            "Average loss for this update: 1.414\n",
            "No Improvement.\n",
            "Epoch  24/100 Batch  180/268 - Loss:  1.406, Seconds: 4.13\n",
            "Epoch  24/100 Batch  200/268 - Loss:  1.409, Seconds: 4.20\n",
            "Epoch  24/100 Batch  220/268 - Loss:  1.426, Seconds: 4.22\n",
            "Epoch  24/100 Batch  240/268 - Loss:  1.408, Seconds: 4.35\n",
            "Epoch  24/100 Batch  260/268 - Loss:  1.435, Seconds: 4.37\n",
            "Average loss for this update: 1.417\n",
            "No Improvement.\n",
            "Epoch  25/100 Batch   20/268 - Loss:  1.363, Seconds: 3.26\n",
            "Epoch  25/100 Batch   40/268 - Loss:  1.303, Seconds: 3.15\n",
            "Epoch  25/100 Batch   60/268 - Loss:  1.366, Seconds: 3.11\n",
            "Epoch  25/100 Batch   80/268 - Loss:  1.302, Seconds: 3.53\n",
            "Average loss for this update: 1.333\n",
            "New Record!\n",
            "Epoch  25/100 Batch  100/268 - Loss:  1.339, Seconds: 3.57\n",
            "Epoch  25/100 Batch  120/268 - Loss:  1.380, Seconds: 3.72\n",
            "Epoch  25/100 Batch  140/268 - Loss:  1.378, Seconds: 3.84\n",
            "Epoch  25/100 Batch  160/268 - Loss:  1.376, Seconds: 4.11\n",
            "Average loss for this update: 1.375\n",
            "No Improvement.\n",
            "Epoch  25/100 Batch  180/268 - Loss:  1.389, Seconds: 4.01\n",
            "Epoch  25/100 Batch  200/268 - Loss:  1.361, Seconds: 4.22\n",
            "Epoch  25/100 Batch  220/268 - Loss:  1.379, Seconds: 4.80\n",
            "Epoch  25/100 Batch  240/268 - Loss:  1.353, Seconds: 4.72\n",
            "Epoch  25/100 Batch  260/268 - Loss:  1.395, Seconds: 4.50\n",
            "Average loss for this update: 1.372\n",
            "No Improvement.\n",
            "Epoch  26/100 Batch   20/268 - Loss:  1.330, Seconds: 2.90\n",
            "Epoch  26/100 Batch   40/268 - Loss:  1.270, Seconds: 3.07\n",
            "Epoch  26/100 Batch   60/268 - Loss:  1.313, Seconds: 3.15\n",
            "Epoch  26/100 Batch   80/268 - Loss:  1.276, Seconds: 3.65\n",
            "Average loss for this update: 1.297\n",
            "New Record!\n",
            "Epoch  26/100 Batch  100/268 - Loss:  1.302, Seconds: 3.61\n",
            "Epoch  26/100 Batch  120/268 - Loss:  1.330, Seconds: 4.09\n",
            "Epoch  26/100 Batch  140/268 - Loss:  1.330, Seconds: 3.62\n",
            "Epoch  26/100 Batch  160/268 - Loss:  1.348, Seconds: 4.09\n",
            "Average loss for this update: 1.333\n",
            "No Improvement.\n",
            "Epoch  26/100 Batch  180/268 - Loss:  1.338, Seconds: 4.56\n",
            "Epoch  26/100 Batch  200/268 - Loss:  1.334, Seconds: 4.07\n",
            "Epoch  26/100 Batch  220/268 - Loss:  1.349, Seconds: 4.76\n",
            "Epoch  26/100 Batch  240/268 - Loss:  1.315, Seconds: 4.57\n",
            "Epoch  26/100 Batch  260/268 - Loss:  1.349, Seconds: 4.97\n",
            "Average loss for this update: 1.335\n",
            "No Improvement.\n",
            "Epoch  27/100 Batch   20/268 - Loss:  1.286, Seconds: 2.96\n",
            "Epoch  27/100 Batch   40/268 - Loss:  1.248, Seconds: 3.14\n",
            "Epoch  27/100 Batch   60/268 - Loss:  1.268, Seconds: 3.31\n",
            "Epoch  27/100 Batch   80/268 - Loss:  1.230, Seconds: 3.50\n",
            "Average loss for this update: 1.258\n",
            "New Record!\n",
            "Epoch  27/100 Batch  100/268 - Loss:  1.262, Seconds: 4.01\n",
            "Epoch  27/100 Batch  120/268 - Loss:  1.295, Seconds: 3.99\n",
            "Epoch  27/100 Batch  140/268 - Loss:  1.288, Seconds: 3.76\n",
            "Epoch  27/100 Batch  160/268 - Loss:  1.296, Seconds: 3.84\n",
            "Average loss for this update: 1.29\n",
            "No Improvement.\n",
            "Epoch  27/100 Batch  180/268 - Loss:  1.304, Seconds: 4.09\n",
            "Epoch  27/100 Batch  200/268 - Loss:  1.293, Seconds: 4.09\n",
            "Epoch  27/100 Batch  220/268 - Loss:  1.308, Seconds: 4.44\n",
            "Epoch  27/100 Batch  240/268 - Loss:  1.298, Seconds: 4.05\n",
            "Epoch  27/100 Batch  260/268 - Loss:  1.290, Seconds: 4.90\n",
            "Average loss for this update: 1.3\n",
            "No Improvement.\n",
            "Epoch  28/100 Batch   20/268 - Loss:  1.236, Seconds: 3.43\n",
            "Epoch  28/100 Batch   40/268 - Loss:  1.213, Seconds: 3.06\n",
            "Epoch  28/100 Batch   60/268 - Loss:  1.226, Seconds: 3.14\n",
            "Epoch  28/100 Batch   80/268 - Loss:  1.196, Seconds: 3.58\n",
            "Average loss for this update: 1.217\n",
            "New Record!\n",
            "Epoch  28/100 Batch  100/268 - Loss:  1.222, Seconds: 3.70\n",
            "Epoch  28/100 Batch  120/268 - Loss:  1.243, Seconds: 3.85\n",
            "Epoch  28/100 Batch  140/268 - Loss:  1.239, Seconds: 3.67\n",
            "Epoch  28/100 Batch  160/268 - Loss:  1.271, Seconds: 3.48\n",
            "Average loss for this update: 1.249\n",
            "No Improvement.\n",
            "Epoch  28/100 Batch  180/268 - Loss:  1.264, Seconds: 3.94\n",
            "Epoch  28/100 Batch  200/268 - Loss:  1.265, Seconds: 4.65\n",
            "Epoch  28/100 Batch  220/268 - Loss:  1.278, Seconds: 4.20\n",
            "Epoch  28/100 Batch  240/268 - Loss:  1.253, Seconds: 4.64\n",
            "Epoch  28/100 Batch  260/268 - Loss:  1.260, Seconds: 4.41\n",
            "Average loss for this update: 1.264\n",
            "No Improvement.\n",
            "Epoch  29/100 Batch   20/268 - Loss:  1.203, Seconds: 2.98\n",
            "Epoch  29/100 Batch   40/268 - Loss:  1.183, Seconds: 3.28\n",
            "Epoch  29/100 Batch   60/268 - Loss:  1.206, Seconds: 3.12\n",
            "Epoch  29/100 Batch   80/268 - Loss:  1.164, Seconds: 3.50\n",
            "Average loss for this update: 1.188\n",
            "New Record!\n",
            "Epoch  29/100 Batch  100/268 - Loss:  1.201, Seconds: 3.99\n",
            "Epoch  29/100 Batch  120/268 - Loss:  1.235, Seconds: 3.79\n",
            "Epoch  29/100 Batch  140/268 - Loss:  1.223, Seconds: 4.12\n",
            "Epoch  29/100 Batch  160/268 - Loss:  1.242, Seconds: 3.57\n",
            "Average loss for this update: 1.232\n",
            "No Improvement.\n",
            "Epoch  29/100 Batch  180/268 - Loss:  1.243, Seconds: 3.96\n",
            "Epoch  29/100 Batch  200/268 - Loss:  1.247, Seconds: 3.97\n",
            "Epoch  29/100 Batch  220/268 - Loss:  1.235, Seconds: 4.31\n",
            "Epoch  29/100 Batch  240/268 - Loss:  1.229, Seconds: 4.17\n",
            "Epoch  29/100 Batch  260/268 - Loss:  1.258, Seconds: 4.69\n",
            "Average loss for this update: 1.242\n",
            "No Improvement.\n",
            "Epoch  30/100 Batch   20/268 - Loss:  1.170, Seconds: 3.15\n",
            "Epoch  30/100 Batch   40/268 - Loss:  1.139, Seconds: 3.11\n",
            "Epoch  30/100 Batch   60/268 - Loss:  1.185, Seconds: 3.14\n",
            "Epoch  30/100 Batch   80/268 - Loss:  1.133, Seconds: 3.54\n",
            "Average loss for this update: 1.158\n",
            "New Record!\n",
            "Epoch  30/100 Batch  100/268 - Loss:  1.183, Seconds: 4.35\n",
            "Epoch  30/100 Batch  120/268 - Loss:  1.196, Seconds: 3.73\n",
            "Epoch  30/100 Batch  140/268 - Loss:  1.181, Seconds: 3.64\n",
            "Epoch  30/100 Batch  160/268 - Loss:  1.217, Seconds: 3.74\n",
            "Average loss for this update: 1.196\n",
            "No Improvement.\n",
            "Epoch  30/100 Batch  180/268 - Loss:  1.201, Seconds: 3.91\n",
            "Epoch  30/100 Batch  200/268 - Loss:  1.222, Seconds: 4.24\n",
            "Epoch  30/100 Batch  220/268 - Loss:  1.215, Seconds: 4.23\n",
            "Epoch  30/100 Batch  240/268 - Loss:  1.199, Seconds: 4.16\n",
            "Epoch  30/100 Batch  260/268 - Loss:  1.217, Seconds: 4.29\n",
            "Average loss for this update: 1.215\n",
            "No Improvement.\n",
            "Epoch  31/100 Batch   20/268 - Loss:  1.159, Seconds: 2.99\n",
            "Epoch  31/100 Batch   40/268 - Loss:  1.096, Seconds: 3.16\n",
            "Epoch  31/100 Batch   60/268 - Loss:  1.152, Seconds: 3.46\n",
            "Epoch  31/100 Batch   80/268 - Loss:  1.112, Seconds: 3.44\n",
            "Average loss for this update: 1.13\n",
            "New Record!\n",
            "Epoch  31/100 Batch  100/268 - Loss:  1.152, Seconds: 3.56\n",
            "Epoch  31/100 Batch  120/268 - Loss:  1.180, Seconds: 3.80\n",
            "Epoch  31/100 Batch  140/268 - Loss:  1.163, Seconds: 3.62\n",
            "Epoch  31/100 Batch  160/268 - Loss:  1.203, Seconds: 3.71\n",
            "Average loss for this update: 1.18\n",
            "No Improvement.\n",
            "Epoch  31/100 Batch  180/268 - Loss:  1.185, Seconds: 3.94\n",
            "Epoch  31/100 Batch  200/268 - Loss:  1.189, Seconds: 4.59\n",
            "Epoch  31/100 Batch  220/268 - Loss:  1.181, Seconds: 4.08\n",
            "Epoch  31/100 Batch  240/268 - Loss:  1.162, Seconds: 4.05\n",
            "Epoch  31/100 Batch  260/268 - Loss:  1.189, Seconds: 4.53\n",
            "Average loss for this update: 1.181\n",
            "No Improvement.\n",
            "Epoch  32/100 Batch   20/268 - Loss:  1.144, Seconds: 3.01\n",
            "Epoch  32/100 Batch   40/268 - Loss:  1.096, Seconds: 3.11\n",
            "Epoch  32/100 Batch   60/268 - Loss:  1.120, Seconds: 3.16\n",
            "Epoch  32/100 Batch   80/268 - Loss:  1.089, Seconds: 3.54\n",
            "Average loss for this update: 1.112\n",
            "New Record!\n",
            "Epoch  32/100 Batch  100/268 - Loss:  1.117, Seconds: 3.61\n",
            "Epoch  32/100 Batch  120/268 - Loss:  1.130, Seconds: 3.68\n",
            "Epoch  32/100 Batch  140/268 - Loss:  1.130, Seconds: 3.66\n",
            "Epoch  32/100 Batch  160/268 - Loss:  1.162, Seconds: 3.71\n",
            "Average loss for this update: 1.14\n",
            "No Improvement.\n",
            "Epoch  32/100 Batch  180/268 - Loss:  1.148, Seconds: 3.94\n",
            "Epoch  32/100 Batch  200/268 - Loss:  1.160, Seconds: 4.32\n",
            "Epoch  32/100 Batch  220/268 - Loss:  1.143, Seconds: 4.51\n",
            "Epoch  32/100 Batch  240/268 - Loss:  1.134, Seconds: 4.00\n",
            "Epoch  32/100 Batch  260/268 - Loss:  1.178, Seconds: 4.39\n",
            "Average loss for this update: 1.154\n",
            "No Improvement.\n",
            "Epoch  33/100 Batch   20/268 - Loss:  1.125, Seconds: 3.26\n",
            "Epoch  33/100 Batch   40/268 - Loss:  1.069, Seconds: 3.47\n",
            "Epoch  33/100 Batch   60/268 - Loss:  1.105, Seconds: 3.11\n",
            "Epoch  33/100 Batch   80/268 - Loss:  1.067, Seconds: 3.92\n",
            "Average loss for this update: 1.091\n",
            "New Record!\n",
            "Epoch  33/100 Batch  100/268 - Loss:  1.104, Seconds: 3.58\n",
            "Epoch  33/100 Batch  120/268 - Loss:  1.122, Seconds: 3.87\n",
            "Epoch  33/100 Batch  140/268 - Loss:  1.111, Seconds: 3.61\n",
            "Epoch  33/100 Batch  160/268 - Loss:  1.137, Seconds: 3.53\n",
            "Average loss for this update: 1.122\n",
            "No Improvement.\n",
            "Epoch  33/100 Batch  180/268 - Loss:  1.123, Seconds: 4.65\n",
            "Epoch  33/100 Batch  200/268 - Loss:  1.146, Seconds: 4.38\n",
            "Epoch  33/100 Batch  220/268 - Loss:  1.140, Seconds: 4.73\n",
            "Epoch  33/100 Batch  240/268 - Loss:  1.112, Seconds: 4.46\n",
            "Epoch  33/100 Batch  260/268 - Loss:  1.133, Seconds: 4.50\n",
            "Average loss for this update: 1.131\n",
            "No Improvement.\n",
            "Epoch  34/100 Batch   20/268 - Loss:  1.067, Seconds: 3.26\n",
            "Epoch  34/100 Batch   40/268 - Loss:  1.029, Seconds: 3.12\n",
            "Epoch  34/100 Batch   60/268 - Loss:  1.082, Seconds: 3.10\n",
            "Epoch  34/100 Batch   80/268 - Loss:  1.047, Seconds: 3.54\n",
            "Average loss for this update: 1.057\n",
            "New Record!\n",
            "Epoch  34/100 Batch  100/268 - Loss:  1.081, Seconds: 3.62\n",
            "Epoch  34/100 Batch  120/268 - Loss:  1.107, Seconds: 3.90\n",
            "Epoch  34/100 Batch  140/268 - Loss:  1.089, Seconds: 3.62\n",
            "Epoch  34/100 Batch  160/268 - Loss:  1.114, Seconds: 3.73\n",
            "Average loss for this update: 1.102\n",
            "No Improvement.\n",
            "Epoch  34/100 Batch  180/268 - Loss:  1.112, Seconds: 3.95\n",
            "Epoch  34/100 Batch  200/268 - Loss:  1.116, Seconds: 4.01\n",
            "Epoch  34/100 Batch  220/268 - Loss:  1.096, Seconds: 4.21\n",
            "Epoch  34/100 Batch  240/268 - Loss:  1.108, Seconds: 4.13\n",
            "Epoch  34/100 Batch  260/268 - Loss:  1.107, Seconds: 4.35\n",
            "Average loss for this update: 1.109\n",
            "No Improvement.\n",
            "Epoch  35/100 Batch   20/268 - Loss:  1.060, Seconds: 3.03\n",
            "Epoch  35/100 Batch   40/268 - Loss:  1.016, Seconds: 3.11\n",
            "Epoch  35/100 Batch   60/268 - Loss:  1.062, Seconds: 3.09\n",
            "Epoch  35/100 Batch   80/268 - Loss:  1.014, Seconds: 3.52\n",
            "Average loss for this update: 1.04\n",
            "New Record!\n",
            "Epoch  35/100 Batch  100/268 - Loss:  1.067, Seconds: 3.79\n",
            "Epoch  35/100 Batch  120/268 - Loss:  1.060, Seconds: 4.15\n",
            "Epoch  35/100 Batch  140/268 - Loss:  1.070, Seconds: 4.13\n",
            "Epoch  35/100 Batch  160/268 - Loss:  1.080, Seconds: 3.55\n",
            "Average loss for this update: 1.074\n",
            "No Improvement.\n",
            "Epoch  35/100 Batch  180/268 - Loss:  1.093, Seconds: 4.59\n",
            "Epoch  35/100 Batch  200/268 - Loss:  1.080, Seconds: 4.07\n",
            "Epoch  35/100 Batch  220/268 - Loss:  1.092, Seconds: 4.77\n",
            "Epoch  35/100 Batch  240/268 - Loss:  1.087, Seconds: 4.09\n",
            "Epoch  35/100 Batch  260/268 - Loss:  1.102, Seconds: 4.66\n",
            "Average loss for this update: 1.09\n",
            "No Improvement.\n",
            "Epoch  36/100 Batch   20/268 - Loss:  1.046, Seconds: 3.10\n",
            "Epoch  36/100 Batch   40/268 - Loss:  1.007, Seconds: 3.09\n",
            "Epoch  36/100 Batch   60/268 - Loss:  1.024, Seconds: 3.11\n",
            "Epoch  36/100 Batch   80/268 - Loss:  1.007, Seconds: 3.46\n",
            "Average loss for this update: 1.022\n",
            "New Record!\n",
            "Epoch  36/100 Batch  100/268 - Loss:  1.038, Seconds: 4.04\n",
            "Epoch  36/100 Batch  120/268 - Loss:  1.051, Seconds: 3.81\n",
            "Epoch  36/100 Batch  140/268 - Loss:  1.055, Seconds: 3.62\n",
            "Epoch  36/100 Batch  160/268 - Loss:  1.076, Seconds: 3.53\n",
            "Average loss for this update: 1.058\n",
            "No Improvement.\n",
            "Epoch  36/100 Batch  180/268 - Loss:  1.058, Seconds: 4.61\n",
            "Epoch  36/100 Batch  200/268 - Loss:  1.052, Seconds: 4.09\n",
            "Epoch  36/100 Batch  220/268 - Loss:  1.071, Seconds: 4.67\n",
            "Epoch  36/100 Batch  240/268 - Loss:  1.053, Seconds: 4.02\n",
            "Epoch  36/100 Batch  260/268 - Loss:  1.056, Seconds: 4.62\n",
            "Average loss for this update: 1.057\n",
            "No Improvement.\n",
            "Epoch  37/100 Batch   20/268 - Loss:  1.018, Seconds: 3.27\n",
            "Epoch  37/100 Batch   40/268 - Loss:  0.982, Seconds: 3.09\n",
            "Epoch  37/100 Batch   60/268 - Loss:  1.025, Seconds: 3.32\n",
            "Epoch  37/100 Batch   80/268 - Loss:  0.984, Seconds: 3.52\n",
            "Average loss for this update: 1.002\n",
            "New Record!\n",
            "Epoch  37/100 Batch  100/268 - Loss:  1.014, Seconds: 3.92\n",
            "Epoch  37/100 Batch  120/268 - Loss:  1.042, Seconds: 4.05\n",
            "Epoch  37/100 Batch  140/268 - Loss:  1.030, Seconds: 3.60\n",
            "Epoch  37/100 Batch  160/268 - Loss:  1.035, Seconds: 3.66\n",
            "Average loss for this update: 1.036\n",
            "No Improvement.\n",
            "Epoch  37/100 Batch  180/268 - Loss:  1.045, Seconds: 3.96\n",
            "Epoch  37/100 Batch  200/268 - Loss:  1.043, Seconds: 4.59\n",
            "Epoch  37/100 Batch  220/268 - Loss:  1.043, Seconds: 4.63\n",
            "Epoch  37/100 Batch  240/268 - Loss:  1.034, Seconds: 4.04\n",
            "Epoch  37/100 Batch  260/268 - Loss:  1.054, Seconds: 4.61\n",
            "Average loss for this update: 1.044\n",
            "No Improvement.\n",
            "Epoch  38/100 Batch   20/268 - Loss:  1.012, Seconds: 3.00\n",
            "Epoch  38/100 Batch   40/268 - Loss:  0.964, Seconds: 3.41\n",
            "Epoch  38/100 Batch   60/268 - Loss:  1.000, Seconds: 3.12\n",
            "Epoch  38/100 Batch   80/268 - Loss:  0.968, Seconds: 3.89\n",
            "Average loss for this update: 0.985\n",
            "New Record!\n",
            "Epoch  38/100 Batch  100/268 - Loss:  1.001, Seconds: 3.61\n",
            "Epoch  38/100 Batch  120/268 - Loss:  1.011, Seconds: 3.96\n",
            "Epoch  38/100 Batch  140/268 - Loss:  1.004, Seconds: 3.62\n",
            "Epoch  38/100 Batch  160/268 - Loss:  1.033, Seconds: 3.53\n",
            "Average loss for this update: 1.017\n",
            "No Improvement.\n",
            "Epoch  38/100 Batch  180/268 - Loss:  1.021, Seconds: 4.14\n",
            "Epoch  38/100 Batch  200/268 - Loss:  1.029, Seconds: 3.96\n",
            "Epoch  38/100 Batch  220/268 - Loss:  1.045, Seconds: 4.22\n",
            "Epoch  38/100 Batch  240/268 - Loss:  1.016, Seconds: 4.11\n",
            "Epoch  38/100 Batch  260/268 - Loss:  1.036, Seconds: 4.31\n",
            "Average loss for this update: 1.031\n",
            "No Improvement.\n",
            "Epoch  39/100 Batch   20/268 - Loss:  0.998, Seconds: 2.98\n",
            "Epoch  39/100 Batch   40/268 - Loss:  0.955, Seconds: 3.15\n",
            "Epoch  39/100 Batch   60/268 - Loss:  0.985, Seconds: 3.14\n",
            "Epoch  39/100 Batch   80/268 - Loss:  0.961, Seconds: 3.49\n",
            "Average loss for this update: 0.976\n",
            "New Record!\n",
            "Epoch  39/100 Batch  100/268 - Loss:  0.986, Seconds: 3.59\n",
            "Epoch  39/100 Batch  120/268 - Loss:  0.994, Seconds: 3.73\n",
            "Epoch  39/100 Batch  140/268 - Loss:  0.994, Seconds: 4.44\n",
            "Epoch  39/100 Batch  160/268 - Loss:  1.008, Seconds: 3.70\n",
            "Average loss for this update: 1.001\n",
            "No Improvement.\n",
            "Epoch  39/100 Batch  180/268 - Loss:  1.017, Seconds: 4.98\n",
            "Epoch  39/100 Batch  200/268 - Loss:  1.025, Seconds: 4.56\n",
            "Epoch  39/100 Batch  220/268 - Loss:  1.016, Seconds: 4.40\n",
            "Epoch  39/100 Batch  240/268 - Loss:  1.007, Seconds: 4.01\n",
            "Epoch  39/100 Batch  260/268 - Loss:  1.011, Seconds: 4.48\n",
            "Average loss for this update: 1.015\n",
            "No Improvement.\n",
            "Epoch  40/100 Batch   20/268 - Loss:  0.984, Seconds: 3.01\n",
            "Epoch  40/100 Batch   40/268 - Loss:  0.942, Seconds: 3.35\n",
            "Epoch  40/100 Batch   60/268 - Loss:  0.975, Seconds: 3.51\n",
            "Epoch  40/100 Batch   80/268 - Loss:  0.923, Seconds: 3.47\n",
            "Average loss for this update: 0.956\n",
            "New Record!\n",
            "Epoch  40/100 Batch  100/268 - Loss:  0.963, Seconds: 3.60\n",
            "Epoch  40/100 Batch  120/268 - Loss:  0.972, Seconds: 3.70\n",
            "Epoch  40/100 Batch  140/268 - Loss:  0.968, Seconds: 4.18\n",
            "Epoch  40/100 Batch  160/268 - Loss:  0.985, Seconds: 3.61\n",
            "Average loss for this update: 0.975\n",
            "No Improvement.\n",
            "Epoch  40/100 Batch  180/268 - Loss:  0.985, Seconds: 4.68\n",
            "Epoch  40/100 Batch  200/268 - Loss:  0.988, Seconds: 3.97\n",
            "Epoch  40/100 Batch  220/268 - Loss:  0.982, Seconds: 4.52\n",
            "Epoch  40/100 Batch  240/268 - Loss:  0.977, Seconds: 4.02\n",
            "Epoch  40/100 Batch  260/268 - Loss:  0.993, Seconds: 4.36\n",
            "Average loss for this update: 0.985\n",
            "No Improvement.\n",
            "Epoch  41/100 Batch   20/268 - Loss:  0.963, Seconds: 3.01\n",
            "Epoch  41/100 Batch   40/268 - Loss:  0.897, Seconds: 3.09\n",
            "Epoch  41/100 Batch   60/268 - Loss:  0.951, Seconds: 3.43\n",
            "Epoch  41/100 Batch   80/268 - Loss:  0.916, Seconds: 3.53\n",
            "Average loss for this update: 0.932\n",
            "New Record!\n",
            "Epoch  41/100 Batch  100/268 - Loss:  0.940, Seconds: 4.02\n",
            "Epoch  41/100 Batch  120/268 - Loss:  0.948, Seconds: 4.00\n",
            "Epoch  41/100 Batch  140/268 - Loss:  0.948, Seconds: 4.03\n",
            "Epoch  41/100 Batch  160/268 - Loss:  0.958, Seconds: 3.88\n",
            "Average loss for this update: 0.95\n",
            "No Improvement.\n",
            "Epoch  41/100 Batch  180/268 - Loss:  0.957, Seconds: 4.32\n",
            "Epoch  41/100 Batch  200/268 - Loss:  0.963, Seconds: 3.99\n",
            "Epoch  41/100 Batch  220/268 - Loss:  0.985, Seconds: 4.09\n",
            "Epoch  41/100 Batch  240/268 - Loss:  0.955, Seconds: 4.17\n",
            "Epoch  41/100 Batch  260/268 - Loss:  0.977, Seconds: 4.31\n",
            "Average loss for this update: 0.971\n",
            "No Improvement.\n",
            "Epoch  42/100 Batch   20/268 - Loss:  0.942, Seconds: 3.21\n",
            "Epoch  42/100 Batch   40/268 - Loss:  0.902, Seconds: 3.08\n",
            "Epoch  42/100 Batch   60/268 - Loss:  0.940, Seconds: 3.14\n",
            "Epoch  42/100 Batch   80/268 - Loss:  0.897, Seconds: 3.52\n",
            "Average loss for this update: 0.919\n",
            "New Record!\n",
            "Epoch  42/100 Batch  100/268 - Loss:  0.921, Seconds: 3.58\n",
            "Epoch  42/100 Batch  120/268 - Loss:  0.943, Seconds: 3.71\n",
            "Epoch  42/100 Batch  140/268 - Loss:  0.927, Seconds: 3.65\n",
            "Epoch  42/100 Batch  160/268 - Loss:  0.943, Seconds: 3.51\n",
            "Average loss for this update: 0.94\n",
            "No Improvement.\n",
            "Epoch  42/100 Batch  180/268 - Loss:  0.954, Seconds: 4.27\n",
            "Epoch  42/100 Batch  200/268 - Loss:  0.943, Seconds: 4.00\n",
            "Epoch  42/100 Batch  220/268 - Loss:  0.969, Seconds: 4.06\n",
            "Epoch  42/100 Batch  240/268 - Loss:  0.952, Seconds: 4.90\n",
            "Epoch  42/100 Batch  260/268 - Loss:  0.958, Seconds: 4.37\n",
            "Average loss for this update: 0.955\n",
            "No Improvement.\n",
            "Epoch  43/100 Batch   20/268 - Loss:  0.920, Seconds: 3.00\n",
            "Epoch  43/100 Batch   40/268 - Loss:  0.880, Seconds: 3.12\n",
            "Epoch  43/100 Batch   60/268 - Loss:  0.927, Seconds: 3.30\n",
            "Epoch  43/100 Batch   80/268 - Loss:  0.876, Seconds: 3.47\n",
            "Average loss for this update: 0.901\n",
            "New Record!\n",
            "Epoch  43/100 Batch  100/268 - Loss:  0.907, Seconds: 3.78\n",
            "Epoch  43/100 Batch  120/268 - Loss:  0.933, Seconds: 4.10\n",
            "Epoch  43/100 Batch  140/268 - Loss:  0.907, Seconds: 3.64\n",
            "Epoch  43/100 Batch  160/268 - Loss:  0.916, Seconds: 3.57\n",
            "Average loss for this update: 0.921\n",
            "No Improvement.\n",
            "Epoch  43/100 Batch  180/268 - Loss:  0.942, Seconds: 4.54\n",
            "Epoch  43/100 Batch  200/268 - Loss:  0.949, Seconds: 4.13\n",
            "Epoch  43/100 Batch  220/268 - Loss:  0.939, Seconds: 4.47\n",
            "Epoch  43/100 Batch  240/268 - Loss:  0.940, Seconds: 4.07\n",
            "Epoch  43/100 Batch  260/268 - Loss:  0.947, Seconds: 4.99\n",
            "Average loss for this update: 0.945\n",
            "No Improvement.\n",
            "Epoch  44/100 Batch   20/268 - Loss:  0.915, Seconds: 3.11\n",
            "Epoch  44/100 Batch   40/268 - Loss:  0.863, Seconds: 3.13\n",
            "Epoch  44/100 Batch   60/268 - Loss:  0.900, Seconds: 3.66\n",
            "Epoch  44/100 Batch   80/268 - Loss:  0.865, Seconds: 3.50\n",
            "Average loss for this update: 0.888\n",
            "New Record!\n",
            "Epoch  44/100 Batch  100/268 - Loss:  0.900, Seconds: 3.60\n",
            "Epoch  44/100 Batch  120/268 - Loss:  0.909, Seconds: 4.16\n",
            "Epoch  44/100 Batch  140/268 - Loss:  0.893, Seconds: 3.69\n",
            "Epoch  44/100 Batch  160/268 - Loss:  0.895, Seconds: 4.14\n",
            "Average loss for this update: 0.902\n",
            "No Improvement.\n",
            "Epoch  44/100 Batch  180/268 - Loss:  0.918, Seconds: 3.91\n",
            "Epoch  44/100 Batch  200/268 - Loss:  0.937, Seconds: 4.03\n",
            "Epoch  44/100 Batch  220/268 - Loss:  0.929, Seconds: 4.14\n",
            "Epoch  44/100 Batch  240/268 - Loss:  0.912, Seconds: 4.05\n",
            "Epoch  44/100 Batch  260/268 - Loss:  0.930, Seconds: 4.85\n",
            "Average loss for this update: 0.926\n",
            "No Improvement.\n",
            "Epoch  45/100 Batch   20/268 - Loss:  0.925, Seconds: 3.28\n",
            "Epoch  45/100 Batch   40/268 - Loss:  0.872, Seconds: 3.40\n",
            "Epoch  45/100 Batch   60/268 - Loss:  0.894, Seconds: 3.52\n",
            "Epoch  45/100 Batch   80/268 - Loss:  0.863, Seconds: 3.53\n",
            "Average loss for this update: 0.885\n",
            "New Record!\n",
            "Epoch  45/100 Batch  100/268 - Loss:  0.875, Seconds: 3.96\n",
            "Epoch  45/100 Batch  120/268 - Loss:  0.902, Seconds: 3.68\n",
            "Epoch  45/100 Batch  140/268 - Loss:  0.893, Seconds: 3.63\n",
            "Epoch  45/100 Batch  160/268 - Loss:  0.906, Seconds: 3.72\n",
            "Average loss for this update: 0.899\n",
            "No Improvement.\n",
            "Epoch  45/100 Batch  180/268 - Loss:  0.904, Seconds: 4.08\n",
            "Epoch  45/100 Batch  200/268 - Loss:  0.927, Seconds: 4.17\n",
            "Epoch  45/100 Batch  220/268 - Loss:  0.909, Seconds: 4.40\n",
            "Epoch  45/100 Batch  240/268 - Loss:  0.906, Seconds: 4.62\n",
            "Epoch  45/100 Batch  260/268 - Loss:  0.904, Seconds: 5.09\n",
            "Average loss for this update: 0.911\n",
            "No Improvement.\n",
            "Epoch  46/100 Batch   20/268 - Loss:  0.925, Seconds: 3.00\n",
            "Epoch  46/100 Batch   40/268 - Loss:  0.849, Seconds: 3.37\n",
            "Epoch  46/100 Batch   60/268 - Loss:  0.886, Seconds: 3.15\n",
            "Epoch  46/100 Batch   80/268 - Loss:  0.853, Seconds: 3.48\n",
            "Average loss for this update: 0.877\n",
            "New Record!\n",
            "Epoch  46/100 Batch  100/268 - Loss:  0.881, Seconds: 4.19\n",
            "Epoch  46/100 Batch  120/268 - Loss:  0.891, Seconds: 3.70\n",
            "Epoch  46/100 Batch  140/268 - Loss:  0.886, Seconds: 3.67\n",
            "Epoch  46/100 Batch  160/268 - Loss:  0.875, Seconds: 3.60\n",
            "Average loss for this update: 0.885\n",
            "No Improvement.\n",
            "Epoch  46/100 Batch  180/268 - Loss:  0.879, Seconds: 3.99\n",
            "Epoch  46/100 Batch  200/268 - Loss:  0.907, Seconds: 4.71\n",
            "Epoch  46/100 Batch  220/268 - Loss:  0.899, Seconds: 4.84\n",
            "Epoch  46/100 Batch  240/268 - Loss:  0.891, Seconds: 4.11\n",
            "Epoch  46/100 Batch  260/268 - Loss:  0.896, Seconds: 4.82\n",
            "Average loss for this update: 0.899\n",
            "No Improvement.\n",
            "Epoch  47/100 Batch   20/268 - Loss:  0.889, Seconds: 3.27\n",
            "Epoch  47/100 Batch   40/268 - Loss:  0.835, Seconds: 3.09\n",
            "Epoch  47/100 Batch   60/268 - Loss:  0.863, Seconds: 3.23\n",
            "Epoch  47/100 Batch   80/268 - Loss:  0.845, Seconds: 3.52\n",
            "Average loss for this update: 0.857\n",
            "New Record!\n",
            "Epoch  47/100 Batch  100/268 - Loss:  0.870, Seconds: 3.67\n",
            "Epoch  47/100 Batch  120/268 - Loss:  0.877, Seconds: 3.75\n",
            "Epoch  47/100 Batch  140/268 - Loss:  0.854, Seconds: 3.64\n",
            "Epoch  47/100 Batch  160/268 - Loss:  0.864, Seconds: 3.53\n",
            "Average loss for this update: 0.863\n",
            "No Improvement.\n",
            "Epoch  47/100 Batch  180/268 - Loss:  0.851, Seconds: 4.47\n",
            "Epoch  47/100 Batch  200/268 - Loss:  0.888, Seconds: 4.22\n",
            "Epoch  47/100 Batch  220/268 - Loss:  0.886, Seconds: 4.23\n",
            "Epoch  47/100 Batch  240/268 - Loss:  0.863, Seconds: 4.64\n",
            "Epoch  47/100 Batch  260/268 - Loss:  0.894, Seconds: 4.37\n",
            "Average loss for this update: 0.882\n",
            "No Improvement.\n",
            "Epoch  48/100 Batch   20/268 - Loss:  0.881, Seconds: 3.07\n",
            "Epoch  48/100 Batch   40/268 - Loss:  0.845, Seconds: 3.20\n",
            "Epoch  48/100 Batch   60/268 - Loss:  0.860, Seconds: 3.61\n",
            "Epoch  48/100 Batch   80/268 - Loss:  0.831, Seconds: 4.03\n",
            "Average loss for this update: 0.852\n",
            "New Record!\n",
            "Epoch  48/100 Batch  100/268 - Loss:  0.841, Seconds: 3.58\n",
            "Epoch  48/100 Batch  120/268 - Loss:  0.854, Seconds: 3.72\n",
            "Epoch  48/100 Batch  140/268 - Loss:  0.854, Seconds: 3.63\n",
            "Epoch  48/100 Batch  160/268 - Loss:  0.848, Seconds: 4.29\n",
            "Average loss for this update: 0.853\n",
            "No Improvement.\n",
            "Epoch  48/100 Batch  180/268 - Loss:  0.859, Seconds: 4.43\n",
            "Epoch  48/100 Batch  200/268 - Loss:  0.870, Seconds: 4.62\n",
            "Epoch  48/100 Batch  220/268 - Loss:  0.884, Seconds: 4.04\n",
            "Epoch  48/100 Batch  240/268 - Loss:  0.873, Seconds: 4.48\n",
            "Epoch  48/100 Batch  260/268 - Loss:  0.867, Seconds: 4.62\n",
            "Average loss for this update: 0.873\n",
            "No Improvement.\n",
            "Epoch  49/100 Batch   20/268 - Loss:  0.854, Seconds: 2.97\n",
            "Epoch  49/100 Batch   40/268 - Loss:  0.815, Seconds: 3.12\n",
            "Epoch  49/100 Batch   60/268 - Loss:  0.842, Seconds: 3.61\n",
            "Epoch  49/100 Batch   80/268 - Loss:  0.812, Seconds: 3.67\n",
            "Average loss for this update: 0.829\n",
            "New Record!\n",
            "Epoch  49/100 Batch  100/268 - Loss:  0.840, Seconds: 4.12\n",
            "Epoch  49/100 Batch  120/268 - Loss:  0.847, Seconds: 3.70\n",
            "Epoch  49/100 Batch  140/268 - Loss:  0.833, Seconds: 4.47\n",
            "Epoch  49/100 Batch  160/268 - Loss:  0.837, Seconds: 3.49\n",
            "Average loss for this update: 0.842\n",
            "No Improvement.\n",
            "Epoch  49/100 Batch  180/268 - Loss:  0.852, Seconds: 4.02\n",
            "Epoch  49/100 Batch  200/268 - Loss:  0.857, Seconds: 4.60\n",
            "Epoch  49/100 Batch  220/268 - Loss:  0.859, Seconds: 4.85\n",
            "Epoch  49/100 Batch  240/268 - Loss:  0.853, Seconds: 4.30\n",
            "Epoch  49/100 Batch  260/268 - Loss:  0.845, Seconds: 4.72\n",
            "Average loss for this update: 0.855\n",
            "No Improvement.\n",
            "Epoch  50/100 Batch   20/268 - Loss:  0.832, Seconds: 3.11\n",
            "Epoch  50/100 Batch   40/268 - Loss:  0.807, Seconds: 3.18\n",
            "Epoch  50/100 Batch   60/268 - Loss:  0.835, Seconds: 3.16\n",
            "Epoch  50/100 Batch   80/268 - Loss:  0.809, Seconds: 3.59\n",
            "Average loss for this update: 0.819\n",
            "New Record!\n",
            "Epoch  50/100 Batch  100/268 - Loss:  0.827, Seconds: 3.70\n",
            "Epoch  50/100 Batch  120/268 - Loss:  0.841, Seconds: 3.79\n",
            "Epoch  50/100 Batch  140/268 - Loss:  0.817, Seconds: 3.67\n",
            "Epoch  50/100 Batch  160/268 - Loss:  0.827, Seconds: 3.57\n",
            "Average loss for this update: 0.833\n",
            "No Improvement.\n",
            "Epoch  50/100 Batch  180/268 - Loss:  0.845, Seconds: 4.42\n",
            "Epoch  50/100 Batch  200/268 - Loss:  0.842, Seconds: 4.04\n",
            "Epoch  50/100 Batch  220/268 - Loss:  0.857, Seconds: 4.32\n",
            "Epoch  50/100 Batch  240/268 - Loss:  0.829, Seconds: 4.36\n",
            "Epoch  50/100 Batch  260/268 - Loss:  0.860, Seconds: 4.33\n",
            "Average loss for this update: 0.847\n",
            "No Improvement.\n",
            "Epoch  51/100 Batch   20/268 - Loss:  0.826, Seconds: 2.98\n",
            "Epoch  51/100 Batch   40/268 - Loss:  0.809, Seconds: 3.40\n",
            "Epoch  51/100 Batch   60/268 - Loss:  0.807, Seconds: 3.17\n",
            "Epoch  51/100 Batch   80/268 - Loss:  0.800, Seconds: 3.52\n",
            "Average loss for this update: 0.808\n",
            "New Record!\n",
            "Epoch  51/100 Batch  100/268 - Loss:  0.816, Seconds: 3.58\n",
            "Epoch  51/100 Batch  120/268 - Loss:  0.824, Seconds: 3.86\n",
            "Epoch  51/100 Batch  140/268 - Loss:  0.826, Seconds: 3.71\n",
            "Epoch  51/100 Batch  160/268 - Loss:  0.828, Seconds: 3.55\n",
            "Average loss for this update: 0.83\n",
            "No Improvement.\n",
            "Epoch  51/100 Batch  180/268 - Loss:  0.837, Seconds: 3.97\n",
            "Epoch  51/100 Batch  200/268 - Loss:  0.838, Seconds: 4.19\n",
            "Epoch  51/100 Batch  220/268 - Loss:  0.836, Seconds: 4.15\n",
            "Epoch  51/100 Batch  240/268 - Loss:  0.834, Seconds: 4.12\n",
            "Epoch  51/100 Batch  260/268 - Loss:  0.838, Seconds: 4.45\n",
            "Average loss for this update: 0.835\n",
            "No Improvement.\n",
            "Epoch  52/100 Batch   20/268 - Loss:  0.847, Seconds: 2.97\n",
            "Epoch  52/100 Batch   40/268 - Loss:  0.787, Seconds: 3.22\n",
            "Epoch  52/100 Batch   60/268 - Loss:  0.825, Seconds: 3.18\n",
            "Epoch  52/100 Batch   80/268 - Loss:  0.778, Seconds: 3.63\n",
            "Average loss for this update: 0.807\n",
            "New Record!\n",
            "Epoch  52/100 Batch  100/268 - Loss:  0.801, Seconds: 3.74\n",
            "Epoch  52/100 Batch  120/268 - Loss:  0.818, Seconds: 3.89\n",
            "Epoch  52/100 Batch  140/268 - Loss:  0.818, Seconds: 4.23\n",
            "Epoch  52/100 Batch  160/268 - Loss:  0.806, Seconds: 3.67\n",
            "Average loss for this update: 0.814\n",
            "No Improvement.\n",
            "Epoch  52/100 Batch  180/268 - Loss:  0.817, Seconds: 3.80\n",
            "Epoch  52/100 Batch  200/268 - Loss:  0.821, Seconds: 4.62\n",
            "Epoch  52/100 Batch  220/268 - Loss:  0.838, Seconds: 4.18\n",
            "Epoch  52/100 Batch  240/268 - Loss:  0.805, Seconds: 4.03\n",
            "Epoch  52/100 Batch  260/268 - Loss:  0.828, Seconds: 4.43\n",
            "Average loss for this update: 0.822\n",
            "No Improvement.\n",
            "Epoch  53/100 Batch   20/268 - Loss:  0.819, Seconds: 3.22\n",
            "Epoch  53/100 Batch   40/268 - Loss:  0.782, Seconds: 3.22\n",
            "Epoch  53/100 Batch   60/268 - Loss:  0.813, Seconds: 3.20\n",
            "Epoch  53/100 Batch   80/268 - Loss:  0.769, Seconds: 3.56\n",
            "Average loss for this update: 0.795\n",
            "New Record!\n",
            "Epoch  53/100 Batch  100/268 - Loss:  0.801, Seconds: 4.09\n",
            "Epoch  53/100 Batch  120/268 - Loss:  0.827, Seconds: 3.67\n",
            "Epoch  53/100 Batch  140/268 - Loss:  0.803, Seconds: 3.63\n",
            "Epoch  53/100 Batch  160/268 - Loss:  0.794, Seconds: 3.74\n",
            "Average loss for this update: 0.808\n",
            "No Improvement.\n",
            "Epoch  53/100 Batch  180/268 - Loss:  0.807, Seconds: 4.03\n",
            "Epoch  53/100 Batch  200/268 - Loss:  0.813, Seconds: 4.05\n",
            "Epoch  53/100 Batch  220/268 - Loss:  0.809, Seconds: 4.30\n",
            "Epoch  53/100 Batch  240/268 - Loss:  0.814, Seconds: 4.08\n",
            "Epoch  53/100 Batch  260/268 - Loss:  0.825, Seconds: 4.64\n",
            "Average loss for this update: 0.816\n",
            "No Improvement.\n",
            "Epoch  54/100 Batch   20/268 - Loss:  0.823, Seconds: 3.05\n",
            "Epoch  54/100 Batch   40/268 - Loss:  0.775, Seconds: 3.11\n",
            "Epoch  54/100 Batch   60/268 - Loss:  0.809, Seconds: 3.18\n",
            "Epoch  54/100 Batch   80/268 - Loss:  0.775, Seconds: 3.73\n",
            "Average loss for this update: 0.795\n",
            "New Record!\n",
            "Epoch  54/100 Batch  100/268 - Loss:  0.794, Seconds: 3.79\n",
            "Epoch  54/100 Batch  120/268 - Loss:  0.809, Seconds: 3.84\n",
            "Epoch  54/100 Batch  140/268 - Loss:  0.795, Seconds: 3.70\n",
            "Epoch  54/100 Batch  160/268 - Loss:  0.798, Seconds: 3.59\n",
            "Average loss for this update: 0.798\n",
            "No Improvement.\n",
            "Epoch  54/100 Batch  180/268 - Loss:  0.786, Seconds: 4.03\n",
            "Epoch  54/100 Batch  200/268 - Loss:  0.799, Seconds: 4.57\n",
            "Epoch  54/100 Batch  220/268 - Loss:  0.814, Seconds: 4.08\n",
            "Epoch  54/100 Batch  240/268 - Loss:  0.796, Seconds: 4.35\n",
            "Epoch  54/100 Batch  260/268 - Loss:  0.809, Seconds: 5.00\n",
            "Average loss for this update: 0.803\n",
            "No Improvement.\n",
            "Epoch  55/100 Batch   20/268 - Loss:  0.794, Seconds: 3.61\n",
            "Epoch  55/100 Batch   40/268 - Loss:  0.766, Seconds: 3.49\n",
            "Epoch  55/100 Batch   60/268 - Loss:  0.781, Seconds: 3.30\n",
            "Epoch  55/100 Batch   80/268 - Loss:  0.768, Seconds: 3.53\n",
            "Average loss for this update: 0.777\n",
            "New Record!\n",
            "Epoch  55/100 Batch  100/268 - Loss:  0.790, Seconds: 3.64\n",
            "Epoch  55/100 Batch  120/268 - Loss:  0.789, Seconds: 4.15\n",
            "Epoch  55/100 Batch  140/268 - Loss:  0.783, Seconds: 3.69\n",
            "Epoch  55/100 Batch  160/268 - Loss:  0.775, Seconds: 3.56\n",
            "Average loss for this update: 0.786\n",
            "No Improvement.\n",
            "Epoch  55/100 Batch  180/268 - Loss:  0.788, Seconds: 4.20\n",
            "Epoch  55/100 Batch  200/268 - Loss:  0.809, Seconds: 4.68\n",
            "Epoch  55/100 Batch  220/268 - Loss:  0.785, Seconds: 4.13\n",
            "Epoch  55/100 Batch  240/268 - Loss:  0.779, Seconds: 4.91\n",
            "Epoch  55/100 Batch  260/268 - Loss:  0.799, Seconds: 4.39\n",
            "Average loss for this update: 0.791\n",
            "No Improvement.\n",
            "Epoch  56/100 Batch   20/268 - Loss:  0.793, Seconds: 3.06\n",
            "Epoch  56/100 Batch   40/268 - Loss:  0.753, Seconds: 3.16\n",
            "Epoch  56/100 Batch   60/268 - Loss:  0.779, Seconds: 3.16\n",
            "Epoch  56/100 Batch   80/268 - Loss:  0.754, Seconds: 3.56\n",
            "Average loss for this update: 0.767\n",
            "New Record!\n",
            "Epoch  56/100 Batch  100/268 - Loss:  0.761, Seconds: 3.88\n",
            "Epoch  56/100 Batch  120/268 - Loss:  0.776, Seconds: 3.69\n",
            "Epoch  56/100 Batch  140/268 - Loss:  0.758, Seconds: 3.76\n",
            "Epoch  56/100 Batch  160/268 - Loss:  0.759, Seconds: 3.79\n",
            "Average loss for this update: 0.769\n",
            "No Improvement.\n",
            "Epoch  56/100 Batch  180/268 - Loss:  0.784, Seconds: 4.30\n",
            "Epoch  56/100 Batch  200/268 - Loss:  0.782, Seconds: 4.08\n",
            "Epoch  56/100 Batch  220/268 - Loss:  0.780, Seconds: 4.08\n",
            "Epoch  56/100 Batch  240/268 - Loss:  0.775, Seconds: 4.02\n",
            "Epoch  56/100 Batch  260/268 - Loss:  0.792, Seconds: 4.65\n",
            "Average loss for this update: 0.783\n",
            "No Improvement.\n",
            "Epoch  57/100 Batch   20/268 - Loss:  0.771, Seconds: 2.98\n",
            "Epoch  57/100 Batch   40/268 - Loss:  0.740, Seconds: 3.44\n",
            "Epoch  57/100 Batch   60/268 - Loss:  0.766, Seconds: 3.20\n",
            "Epoch  57/100 Batch   80/268 - Loss:  0.734, Seconds: 3.58\n",
            "Average loss for this update: 0.753\n",
            "New Record!\n",
            "Epoch  57/100 Batch  100/268 - Loss:  0.759, Seconds: 3.61\n",
            "Epoch  57/100 Batch  120/268 - Loss:  0.770, Seconds: 4.09\n",
            "Epoch  57/100 Batch  140/268 - Loss:  0.762, Seconds: 3.69\n",
            "Epoch  57/100 Batch  160/268 - Loss:  0.766, Seconds: 4.23\n",
            "Average loss for this update: 0.766\n",
            "No Improvement.\n",
            "Epoch  57/100 Batch  180/268 - Loss:  0.775, Seconds: 3.97\n",
            "Epoch  57/100 Batch  200/268 - Loss:  0.776, Seconds: 4.02\n",
            "Epoch  57/100 Batch  220/268 - Loss:  0.780, Seconds: 4.76\n",
            "Epoch  57/100 Batch  240/268 - Loss:  0.763, Seconds: 4.01\n",
            "Epoch  57/100 Batch  260/268 - Loss:  0.778, Seconds: 4.39\n",
            "Average loss for this update: 0.773\n",
            "No Improvement.\n",
            "Epoch  58/100 Batch   20/268 - Loss:  0.766, Seconds: 3.27\n",
            "Epoch  58/100 Batch   40/268 - Loss:  0.742, Seconds: 3.10\n",
            "Epoch  58/100 Batch   60/268 - Loss:  0.762, Seconds: 3.55\n",
            "Epoch  58/100 Batch   80/268 - Loss:  0.736, Seconds: 3.86\n",
            "Average loss for this update: 0.752\n",
            "New Record!\n",
            "Epoch  58/100 Batch  100/268 - Loss:  0.755, Seconds: 3.94\n",
            "Epoch  58/100 Batch  120/268 - Loss:  0.760, Seconds: 3.82\n",
            "Epoch  58/100 Batch  140/268 - Loss:  0.747, Seconds: 3.76\n",
            "Epoch  58/100 Batch  160/268 - Loss:  0.755, Seconds: 3.66\n",
            "Average loss for this update: 0.754\n",
            "No Improvement.\n",
            "Epoch  58/100 Batch  180/268 - Loss:  0.750, Seconds: 4.11\n",
            "Epoch  58/100 Batch  200/268 - Loss:  0.775, Seconds: 3.97\n",
            "Epoch  58/100 Batch  220/268 - Loss:  0.768, Seconds: 4.06\n",
            "Epoch  58/100 Batch  240/268 - Loss:  0.770, Seconds: 4.11\n",
            "Epoch  58/100 Batch  260/268 - Loss:  0.767, Seconds: 4.55\n",
            "Average loss for this update: 0.769\n",
            "No Improvement.\n",
            "Epoch  59/100 Batch   20/268 - Loss:  0.754, Seconds: 2.99\n",
            "Epoch  59/100 Batch   40/268 - Loss:  0.715, Seconds: 3.25\n",
            "Epoch  59/100 Batch   60/268 - Loss:  0.771, Seconds: 3.79\n",
            "Epoch  59/100 Batch   80/268 - Loss:  0.721, Seconds: 3.84\n",
            "Average loss for this update: 0.74\n",
            "New Record!\n",
            "Epoch  59/100 Batch  100/268 - Loss:  0.753, Seconds: 3.57\n",
            "Epoch  59/100 Batch  120/268 - Loss:  0.757, Seconds: 4.17\n",
            "Epoch  59/100 Batch  140/268 - Loss:  0.746, Seconds: 4.12\n",
            "Epoch  59/100 Batch  160/268 - Loss:  0.749, Seconds: 3.82\n",
            "Average loss for this update: 0.758\n",
            "No Improvement.\n",
            "Epoch  59/100 Batch  180/268 - Loss:  0.777, Seconds: 4.47\n",
            "Epoch  59/100 Batch  200/268 - Loss:  0.778, Seconds: 4.31\n",
            "Epoch  59/100 Batch  220/268 - Loss:  0.768, Seconds: 4.12\n",
            "Epoch  59/100 Batch  240/268 - Loss:  0.767, Seconds: 4.17\n",
            "Epoch  59/100 Batch  260/268 - Loss:  0.769, Seconds: 4.44\n",
            "Average loss for this update: 0.771\n",
            "No Improvement.\n",
            "Epoch  60/100 Batch   20/268 - Loss:  0.739, Seconds: 3.40\n",
            "Epoch  60/100 Batch   40/268 - Loss:  0.721, Seconds: 3.09\n",
            "Epoch  60/100 Batch   60/268 - Loss:  0.753, Seconds: 3.26\n",
            "Epoch  60/100 Batch   80/268 - Loss:  0.721, Seconds: 3.68\n",
            "Average loss for this update: 0.734\n",
            "New Record!\n",
            "Epoch  60/100 Batch  100/268 - Loss:  0.735, Seconds: 3.81\n",
            "Epoch  60/100 Batch  120/268 - Loss:  0.766, Seconds: 4.03\n",
            "Epoch  60/100 Batch  140/268 - Loss:  0.744, Seconds: 4.03\n",
            "Epoch  60/100 Batch  160/268 - Loss:  0.758, Seconds: 3.65\n",
            "Average loss for this update: 0.75\n",
            "No Improvement.\n",
            "Epoch  60/100 Batch  180/268 - Loss:  0.739, Seconds: 4.13\n",
            "Epoch  60/100 Batch  200/268 - Loss:  0.753, Seconds: 4.08\n",
            "Epoch  60/100 Batch  220/268 - Loss:  0.761, Seconds: 4.23\n",
            "Epoch  60/100 Batch  240/268 - Loss:  0.751, Seconds: 4.15\n",
            "Epoch  60/100 Batch  260/268 - Loss:  0.742, Seconds: 4.49\n",
            "Average loss for this update: 0.752\n",
            "No Improvement.\n",
            "Epoch  61/100 Batch   20/268 - Loss:  0.731, Seconds: 3.00\n",
            "Epoch  61/100 Batch   40/268 - Loss:  0.713, Seconds: 3.13\n",
            "Epoch  61/100 Batch   60/268 - Loss:  0.740, Seconds: 3.42\n",
            "Epoch  61/100 Batch   80/268 - Loss:  0.738, Seconds: 3.88\n",
            "Average loss for this update: 0.73\n",
            "New Record!\n",
            "Epoch  61/100 Batch  100/268 - Loss:  0.740, Seconds: 3.70\n",
            "Epoch  61/100 Batch  120/268 - Loss:  0.725, Seconds: 3.84\n",
            "Epoch  61/100 Batch  140/268 - Loss:  0.743, Seconds: 3.69\n",
            "Epoch  61/100 Batch  160/268 - Loss:  0.731, Seconds: 3.58\n",
            "Average loss for this update: 0.738\n",
            "No Improvement.\n",
            "Epoch  61/100 Batch  180/268 - Loss:  0.745, Seconds: 3.92\n",
            "Epoch  61/100 Batch  200/268 - Loss:  0.742, Seconds: 4.00\n",
            "Epoch  61/100 Batch  220/268 - Loss:  0.749, Seconds: 4.23\n",
            "Epoch  61/100 Batch  240/268 - Loss:  0.736, Seconds: 4.24\n",
            "Epoch  61/100 Batch  260/268 - Loss:  0.746, Seconds: 4.42\n",
            "Average loss for this update: 0.743\n",
            "No Improvement.\n",
            "Epoch  62/100 Batch   20/268 - Loss:  0.724, Seconds: 2.99\n",
            "Epoch  62/100 Batch   40/268 - Loss:  0.698, Seconds: 3.49\n",
            "Epoch  62/100 Batch   60/268 - Loss:  0.741, Seconds: 3.11\n",
            "Epoch  62/100 Batch   80/268 - Loss:  0.704, Seconds: 3.52\n",
            "Average loss for this update: 0.717\n",
            "New Record!\n",
            "Epoch  62/100 Batch  100/268 - Loss:  0.737, Seconds: 4.17\n",
            "Epoch  62/100 Batch  120/268 - Loss:  0.722, Seconds: 3.74\n",
            "Epoch  62/100 Batch  140/268 - Loss:  0.730, Seconds: 4.22\n",
            "Epoch  62/100 Batch  160/268 - Loss:  0.729, Seconds: 3.56\n",
            "Average loss for this update: 0.727\n",
            "No Improvement.\n",
            "Epoch  62/100 Batch  180/268 - Loss:  0.724, Seconds: 4.10\n",
            "Epoch  62/100 Batch  200/268 - Loss:  0.726, Seconds: 4.00\n",
            "Epoch  62/100 Batch  220/268 - Loss:  0.745, Seconds: 4.06\n",
            "Epoch  62/100 Batch  240/268 - Loss:  0.729, Seconds: 4.16\n",
            "Epoch  62/100 Batch  260/268 - Loss:  0.744, Seconds: 4.33\n",
            "Average loss for this update: 0.738\n",
            "No Improvement.\n",
            "Epoch  63/100 Batch   20/268 - Loss:  0.723, Seconds: 3.23\n",
            "Epoch  63/100 Batch   40/268 - Loss:  0.677, Seconds: 3.07\n",
            "Epoch  63/100 Batch   60/268 - Loss:  0.722, Seconds: 3.11\n",
            "Epoch  63/100 Batch   80/268 - Loss:  0.705, Seconds: 3.59\n",
            "Average loss for this update: 0.709\n",
            "New Record!\n",
            "Epoch  63/100 Batch  100/268 - Loss:  0.716, Seconds: 3.58\n",
            "Epoch  63/100 Batch  120/268 - Loss:  0.737, Seconds: 3.71\n",
            "Epoch  63/100 Batch  140/268 - Loss:  0.717, Seconds: 3.61\n",
            "Epoch  63/100 Batch  160/268 - Loss:  0.723, Seconds: 3.68\n",
            "Average loss for this update: 0.721\n",
            "No Improvement.\n",
            "Epoch  63/100 Batch  180/268 - Loss:  0.712, Seconds: 4.53\n",
            "Epoch  63/100 Batch  200/268 - Loss:  0.716, Seconds: 3.97\n",
            "Epoch  63/100 Batch  220/268 - Loss:  0.734, Seconds: 4.64\n",
            "Epoch  63/100 Batch  240/268 - Loss:  0.732, Seconds: 4.05\n",
            "Epoch  63/100 Batch  260/268 - Loss:  0.736, Seconds: 4.39\n",
            "Average loss for this update: 0.729\n",
            "No Improvement.\n",
            "Epoch  64/100 Batch   20/268 - Loss:  0.721, Seconds: 3.00\n",
            "Epoch  64/100 Batch   40/268 - Loss:  0.691, Seconds: 3.12\n",
            "Epoch  64/100 Batch   60/268 - Loss:  0.718, Seconds: 3.16\n",
            "Epoch  64/100 Batch   80/268 - Loss:  0.671, Seconds: 3.54\n",
            "Average loss for this update: 0.701\n",
            "New Record!\n",
            "Epoch  64/100 Batch  100/268 - Loss:  0.717, Seconds: 3.84\n",
            "Epoch  64/100 Batch  120/268 - Loss:  0.726, Seconds: 3.78\n",
            "Epoch  64/100 Batch  140/268 - Loss:  0.710, Seconds: 3.92\n",
            "Epoch  64/100 Batch  160/268 - Loss:  0.707, Seconds: 3.55\n",
            "Average loss for this update: 0.716\n",
            "No Improvement.\n",
            "Epoch  64/100 Batch  180/268 - Loss:  0.717, Seconds: 3.95\n",
            "Epoch  64/100 Batch  200/268 - Loss:  0.718, Seconds: 3.98\n",
            "Epoch  64/100 Batch  220/268 - Loss:  0.716, Seconds: 4.09\n",
            "Epoch  64/100 Batch  240/268 - Loss:  0.712, Seconds: 4.20\n",
            "Epoch  64/100 Batch  260/268 - Loss:  0.727, Seconds: 4.55\n",
            "Average loss for this update: 0.717\n",
            "No Improvement.\n",
            "Epoch  65/100 Batch   20/268 - Loss:  0.705, Seconds: 3.12\n",
            "Epoch  65/100 Batch   40/268 - Loss:  0.671, Seconds: 3.05\n",
            "Epoch  65/100 Batch   60/268 - Loss:  0.708, Seconds: 3.43\n",
            "Epoch  65/100 Batch   80/268 - Loss:  0.669, Seconds: 4.00\n",
            "Average loss for this update: 0.688\n",
            "New Record!\n",
            "Epoch  65/100 Batch  100/268 - Loss:  0.685, Seconds: 3.83\n",
            "Epoch  65/100 Batch  120/268 - Loss:  0.719, Seconds: 3.72\n",
            "Epoch  65/100 Batch  140/268 - Loss:  0.697, Seconds: 3.64\n",
            "Epoch  65/100 Batch  160/268 - Loss:  0.693, Seconds: 3.60\n",
            "Average loss for this update: 0.701\n",
            "No Improvement.\n",
            "Epoch  65/100 Batch  180/268 - Loss:  0.713, Seconds: 4.12\n",
            "Epoch  65/100 Batch  200/268 - Loss:  0.691, Seconds: 4.18\n",
            "Epoch  65/100 Batch  220/268 - Loss:  0.709, Seconds: 4.21\n",
            "Epoch  65/100 Batch  240/268 - Loss:  0.708, Seconds: 4.60\n",
            "Epoch  65/100 Batch  260/268 - Loss:  0.703, Seconds: 4.84\n",
            "Average loss for this update: 0.707\n",
            "No Improvement.\n",
            "Epoch  66/100 Batch   20/268 - Loss:  0.681, Seconds: 3.41\n",
            "Epoch  66/100 Batch   40/268 - Loss:  0.667, Seconds: 3.12\n",
            "Epoch  66/100 Batch   60/268 - Loss:  0.695, Seconds: 3.15\n",
            "Epoch  66/100 Batch   80/268 - Loss:  0.653, Seconds: 3.71\n",
            "Average loss for this update: 0.675\n",
            "New Record!\n",
            "Epoch  66/100 Batch  100/268 - Loss:  0.686, Seconds: 3.83\n",
            "Epoch  66/100 Batch  120/268 - Loss:  0.699, Seconds: 4.15\n",
            "Epoch  66/100 Batch  140/268 - Loss:  0.701, Seconds: 4.08\n",
            "Epoch  66/100 Batch  160/268 - Loss:  0.683, Seconds: 3.55\n",
            "Average loss for this update: 0.693\n",
            "No Improvement.\n",
            "Epoch  66/100 Batch  180/268 - Loss:  0.694, Seconds: 4.05\n",
            "Epoch  66/100 Batch  200/268 - Loss:  0.693, Seconds: 4.31\n",
            "Epoch  66/100 Batch  220/268 - Loss:  0.692, Seconds: 4.13\n",
            "Epoch  66/100 Batch  240/268 - Loss:  0.697, Seconds: 4.72\n",
            "Epoch  66/100 Batch  260/268 - Loss:  0.715, Seconds: 4.64\n",
            "Average loss for this update: 0.698\n",
            "No Improvement.\n",
            "Epoch  67/100 Batch   20/268 - Loss:  0.699, Seconds: 3.16\n",
            "Epoch  67/100 Batch   40/268 - Loss:  0.668, Seconds: 3.15\n",
            "Epoch  67/100 Batch   60/268 - Loss:  0.710, Seconds: 3.11\n",
            "Epoch  67/100 Batch   80/268 - Loss:  0.666, Seconds: 3.76\n",
            "Average loss for this update: 0.686\n",
            "No Improvement.\n",
            "Epoch  67/100 Batch  100/268 - Loss:  0.686, Seconds: 4.00\n",
            "Epoch  67/100 Batch  120/268 - Loss:  0.706, Seconds: 4.33\n",
            "Epoch  67/100 Batch  140/268 - Loss:  0.703, Seconds: 3.71\n",
            "Epoch  67/100 Batch  160/268 - Loss:  0.684, Seconds: 3.81\n",
            "Average loss for this update: 0.696\n",
            "No Improvement.\n",
            "Epoch  67/100 Batch  180/268 - Loss:  0.695, Seconds: 3.94\n",
            "Epoch  67/100 Batch  200/268 - Loss:  0.705, Seconds: 4.33\n",
            "Epoch  67/100 Batch  220/268 - Loss:  0.691, Seconds: 4.65\n",
            "Epoch  67/100 Batch  240/268 - Loss:  0.688, Seconds: 4.05\n",
            "Epoch  67/100 Batch  260/268 - Loss:  0.707, Seconds: 4.48\n",
            "Average loss for this update: 0.697\n",
            "No Improvement.\n",
            "Epoch  68/100 Batch   20/268 - Loss:  0.702, Seconds: 3.18\n",
            "Epoch  68/100 Batch   40/268 - Loss:  0.646, Seconds: 3.20\n",
            "Epoch  68/100 Batch   60/268 - Loss:  0.683, Seconds: 3.16\n",
            "Epoch  68/100 Batch   80/268 - Loss:  0.653, Seconds: 3.53\n",
            "Average loss for this update: 0.673\n",
            "New Record!\n",
            "Epoch  68/100 Batch  100/268 - Loss:  0.685, Seconds: 3.77\n",
            "Epoch  68/100 Batch  120/268 - Loss:  0.683, Seconds: 3.69\n",
            "Epoch  68/100 Batch  140/268 - Loss:  0.695, Seconds: 3.74\n",
            "Epoch  68/100 Batch  160/268 - Loss:  0.694, Seconds: 3.92\n",
            "Average loss for this update: 0.688\n",
            "No Improvement.\n",
            "Epoch  68/100 Batch  180/268 - Loss:  0.681, Seconds: 3.91\n",
            "Epoch  68/100 Batch  200/268 - Loss:  0.695, Seconds: 4.02\n",
            "Epoch  68/100 Batch  220/268 - Loss:  0.704, Seconds: 4.12\n",
            "Epoch  68/100 Batch  240/268 - Loss:  0.686, Seconds: 4.07\n",
            "Epoch  68/100 Batch  260/268 - Loss:  0.686, Seconds: 4.77\n",
            "Average loss for this update: 0.693\n",
            "No Improvement.\n",
            "Epoch  69/100 Batch   20/268 - Loss:  0.702, Seconds: 3.03\n",
            "Epoch  69/100 Batch   40/268 - Loss:  0.652, Seconds: 3.43\n",
            "Epoch  69/100 Batch   60/268 - Loss:  0.693, Seconds: 3.19\n",
            "Epoch  69/100 Batch   80/268 - Loss:  0.658, Seconds: 3.56\n",
            "Average loss for this update: 0.676\n",
            "No Improvement.\n",
            "Epoch  69/100 Batch  100/268 - Loss:  0.667, Seconds: 3.79\n",
            "Epoch  69/100 Batch  120/268 - Loss:  0.683, Seconds: 3.76\n",
            "Epoch  69/100 Batch  140/268 - Loss:  0.675, Seconds: 3.67\n",
            "Epoch  69/100 Batch  160/268 - Loss:  0.661, Seconds: 3.70\n",
            "Average loss for this update: 0.671\n",
            "New Record!\n",
            "Epoch  69/100 Batch  180/268 - Loss:  0.675, Seconds: 4.07\n",
            "Epoch  69/100 Batch  200/268 - Loss:  0.685, Seconds: 4.64\n",
            "Epoch  69/100 Batch  220/268 - Loss:  0.689, Seconds: 4.68\n",
            "Epoch  69/100 Batch  240/268 - Loss:  0.671, Seconds: 4.15\n",
            "Epoch  69/100 Batch  260/268 - Loss:  0.684, Seconds: 4.58\n",
            "Average loss for this update: 0.683\n",
            "No Improvement.\n",
            "Epoch  70/100 Batch   20/268 - Loss:  0.671, Seconds: 3.15\n",
            "Epoch  70/100 Batch   40/268 - Loss:  0.647, Seconds: 3.47\n",
            "Epoch  70/100 Batch   60/268 - Loss:  0.669, Seconds: 3.15\n",
            "Epoch  70/100 Batch   80/268 - Loss:  0.647, Seconds: 3.81\n",
            "Average loss for this update: 0.658\n",
            "New Record!\n",
            "Epoch  70/100 Batch  100/268 - Loss:  0.652, Seconds: 3.59\n",
            "Epoch  70/100 Batch  120/268 - Loss:  0.672, Seconds: 4.37\n",
            "Epoch  70/100 Batch  140/268 - Loss:  0.673, Seconds: 3.88\n",
            "Epoch  70/100 Batch  160/268 - Loss:  0.667, Seconds: 4.06\n",
            "Average loss for this update: 0.666\n",
            "No Improvement.\n",
            "Epoch  70/100 Batch  180/268 - Loss:  0.669, Seconds: 4.82\n",
            "Epoch  70/100 Batch  200/268 - Loss:  0.666, Seconds: 4.11\n",
            "Epoch  70/100 Batch  220/268 - Loss:  0.689, Seconds: 4.66\n",
            "Epoch  70/100 Batch  240/268 - Loss:  0.672, Seconds: 4.64\n",
            "Epoch  70/100 Batch  260/268 - Loss:  0.685, Seconds: 4.50\n",
            "Average loss for this update: 0.678\n",
            "No Improvement.\n",
            "Epoch  71/100 Batch   20/268 - Loss:  0.692, Seconds: 2.97\n",
            "Epoch  71/100 Batch   40/268 - Loss:  0.630, Seconds: 3.11\n",
            "Epoch  71/100 Batch   60/268 - Loss:  0.683, Seconds: 3.18\n",
            "Epoch  71/100 Batch   80/268 - Loss:  0.652, Seconds: 3.51\n",
            "Average loss for this update: 0.664\n",
            "No Improvement.\n",
            "Epoch  71/100 Batch  100/268 - Loss:  0.657, Seconds: 3.62\n",
            "Epoch  71/100 Batch  120/268 - Loss:  0.682, Seconds: 3.88\n",
            "Epoch  71/100 Batch  140/268 - Loss:  0.676, Seconds: 4.37\n",
            "Epoch  71/100 Batch  160/268 - Loss:  0.665, Seconds: 3.53\n",
            "Average loss for this update: 0.672\n",
            "No Improvement.\n",
            "Epoch  71/100 Batch  180/268 - Loss:  0.679, Seconds: 3.91\n",
            "Epoch  71/100 Batch  200/268 - Loss:  0.686, Seconds: 4.35\n",
            "Epoch  71/100 Batch  220/268 - Loss:  0.678, Seconds: 4.77\n",
            "Epoch  71/100 Batch  240/268 - Loss:  0.674, Seconds: 4.13\n",
            "Epoch  71/100 Batch  260/268 - Loss:  0.679, Seconds: 4.37\n",
            "Average loss for this update: 0.68\n",
            "No Improvement.\n",
            "Epoch  72/100 Batch   20/268 - Loss:  0.665, Seconds: 3.00\n",
            "Epoch  72/100 Batch   40/268 - Loss:  0.647, Seconds: 3.12\n",
            "Epoch  72/100 Batch   60/268 - Loss:  0.656, Seconds: 3.29\n",
            "Epoch  72/100 Batch   80/268 - Loss:  0.638, Seconds: 3.52\n",
            "Average loss for this update: 0.654\n",
            "New Record!\n",
            "Epoch  72/100 Batch  100/268 - Loss:  0.671, Seconds: 3.61\n",
            "Epoch  72/100 Batch  120/268 - Loss:  0.655, Seconds: 4.34\n",
            "Epoch  72/100 Batch  140/268 - Loss:  0.665, Seconds: 3.66\n",
            "Epoch  72/100 Batch  160/268 - Loss:  0.650, Seconds: 4.05\n",
            "Average loss for this update: 0.657\n",
            "No Improvement.\n",
            "Epoch  72/100 Batch  180/268 - Loss:  0.653, Seconds: 3.94\n",
            "Epoch  72/100 Batch  200/268 - Loss:  0.662, Seconds: 3.99\n",
            "Epoch  72/100 Batch  220/268 - Loss:  0.678, Seconds: 4.20\n",
            "Epoch  72/100 Batch  240/268 - Loss:  0.650, Seconds: 4.77\n",
            "Epoch  72/100 Batch  260/268 - Loss:  0.675, Seconds: 4.39\n",
            "Average loss for this update: 0.666\n",
            "No Improvement.\n",
            "Epoch  73/100 Batch   20/268 - Loss:  0.644, Seconds: 2.98\n",
            "Epoch  73/100 Batch   40/268 - Loss:  0.637, Seconds: 3.08\n",
            "Epoch  73/100 Batch   60/268 - Loss:  0.655, Seconds: 3.58\n",
            "Epoch  73/100 Batch   80/268 - Loss:  0.639, Seconds: 3.51\n",
            "Average loss for this update: 0.644\n",
            "New Record!\n",
            "Epoch  73/100 Batch  100/268 - Loss:  0.649, Seconds: 4.08\n",
            "Epoch  73/100 Batch  120/268 - Loss:  0.652, Seconds: 3.90\n",
            "Epoch  73/100 Batch  140/268 - Loss:  0.659, Seconds: 3.66\n",
            "Epoch  73/100 Batch  160/268 - Loss:  0.647, Seconds: 3.58\n",
            "Average loss for this update: 0.652\n",
            "No Improvement.\n",
            "Epoch  73/100 Batch  180/268 - Loss:  0.650, Seconds: 3.95\n",
            "Epoch  73/100 Batch  200/268 - Loss:  0.651, Seconds: 4.01\n",
            "Epoch  73/100 Batch  220/268 - Loss:  0.660, Seconds: 4.12\n",
            "Epoch  73/100 Batch  240/268 - Loss:  0.664, Seconds: 4.13\n",
            "Epoch  73/100 Batch  260/268 - Loss:  0.669, Seconds: 4.62\n",
            "Average loss for this update: 0.661\n",
            "No Improvement.\n",
            "Epoch  74/100 Batch   20/268 - Loss:  0.652, Seconds: 3.06\n",
            "Epoch  74/100 Batch   40/268 - Loss:  0.622, Seconds: 3.13\n",
            "Epoch  74/100 Batch   60/268 - Loss:  0.659, Seconds: 3.40\n",
            "Epoch  74/100 Batch   80/268 - Loss:  0.627, Seconds: 3.76\n",
            "Average loss for this update: 0.639\n",
            "New Record!\n",
            "Epoch  74/100 Batch  100/268 - Loss:  0.645, Seconds: 3.75\n",
            "Epoch  74/100 Batch  120/268 - Loss:  0.651, Seconds: 3.70\n",
            "Epoch  74/100 Batch  140/268 - Loss:  0.643, Seconds: 3.80\n",
            "Epoch  74/100 Batch  160/268 - Loss:  0.650, Seconds: 3.55\n",
            "Average loss for this update: 0.648\n",
            "No Improvement.\n",
            "Epoch  74/100 Batch  180/268 - Loss:  0.652, Seconds: 4.66\n",
            "Epoch  74/100 Batch  200/268 - Loss:  0.647, Seconds: 4.57\n",
            "Epoch  74/100 Batch  220/268 - Loss:  0.656, Seconds: 4.76\n",
            "Epoch  74/100 Batch  240/268 - Loss:  0.643, Seconds: 4.09\n",
            "Epoch  74/100 Batch  260/268 - Loss:  0.655, Seconds: 4.41\n",
            "Average loss for this update: 0.652\n",
            "No Improvement.\n",
            "Epoch  75/100 Batch   20/268 - Loss:  0.671, Seconds: 3.04\n",
            "Epoch  75/100 Batch   40/268 - Loss:  0.604, Seconds: 3.12\n",
            "Epoch  75/100 Batch   60/268 - Loss:  0.638, Seconds: 3.29\n",
            "Epoch  75/100 Batch   80/268 - Loss:  0.630, Seconds: 3.84\n",
            "Average loss for this update: 0.637\n",
            "New Record!\n",
            "Epoch  75/100 Batch  100/268 - Loss:  0.640, Seconds: 3.60\n",
            "Epoch  75/100 Batch  120/268 - Loss:  0.665, Seconds: 3.77\n",
            "Epoch  75/100 Batch  140/268 - Loss:  0.639, Seconds: 4.01\n",
            "Epoch  75/100 Batch  160/268 - Loss:  0.652, Seconds: 3.54\n",
            "Average loss for this update: 0.65\n",
            "No Improvement.\n",
            "Epoch  75/100 Batch  180/268 - Loss:  0.658, Seconds: 4.64\n",
            "Epoch  75/100 Batch  200/268 - Loss:  0.653, Seconds: 4.20\n",
            "Epoch  75/100 Batch  220/268 - Loss:  0.655, Seconds: 4.07\n",
            "Epoch  75/100 Batch  240/268 - Loss:  0.657, Seconds: 4.22\n",
            "Epoch  75/100 Batch  260/268 - Loss:  0.653, Seconds: 4.49\n",
            "Average loss for this update: 0.655\n",
            "No Improvement.\n",
            "Epoch  76/100 Batch   20/268 - Loss:  0.661, Seconds: 3.02\n",
            "Epoch  76/100 Batch   40/268 - Loss:  0.611, Seconds: 3.14\n",
            "Epoch  76/100 Batch   60/268 - Loss:  0.660, Seconds: 3.27\n",
            "Epoch  76/100 Batch   80/268 - Loss:  0.631, Seconds: 3.91\n",
            "Average loss for this update: 0.639\n",
            "No Improvement.\n",
            "Epoch  76/100 Batch  100/268 - Loss:  0.635, Seconds: 3.65\n",
            "Epoch  76/100 Batch  120/268 - Loss:  0.655, Seconds: 3.75\n",
            "Epoch  76/100 Batch  140/268 - Loss:  0.630, Seconds: 3.79\n",
            "Epoch  76/100 Batch  160/268 - Loss:  0.629, Seconds: 3.62\n",
            "Average loss for this update: 0.643\n",
            "No Improvement.\n",
            "Epoch  76/100 Batch  180/268 - Loss:  0.662, Seconds: 3.98\n",
            "Epoch  76/100 Batch  200/268 - Loss:  0.657, Seconds: 4.16\n",
            "Epoch  76/100 Batch  220/268 - Loss:  0.656, Seconds: 4.15\n",
            "Epoch  76/100 Batch  240/268 - Loss:  0.659, Seconds: 4.17\n",
            "Epoch  76/100 Batch  260/268 - Loss:  0.637, Seconds: 4.33\n",
            "Average loss for this update: 0.653\n",
            "No Improvement.\n",
            "Epoch  77/100 Batch   20/268 - Loss:  0.626, Seconds: 3.02\n",
            "Epoch  77/100 Batch   40/268 - Loss:  0.607, Seconds: 3.31\n",
            "Epoch  77/100 Batch   60/268 - Loss:  0.635, Seconds: 3.20\n",
            "Epoch  77/100 Batch   80/268 - Loss:  0.614, Seconds: 3.55\n",
            "Average loss for this update: 0.621\n",
            "New Record!\n",
            "Epoch  77/100 Batch  100/268 - Loss:  0.637, Seconds: 3.62\n",
            "Epoch  77/100 Batch  120/268 - Loss:  0.639, Seconds: 3.92\n",
            "Epoch  77/100 Batch  140/268 - Loss:  0.627, Seconds: 3.67\n",
            "Epoch  77/100 Batch  160/268 - Loss:  0.628, Seconds: 3.70\n",
            "Average loss for this update: 0.634\n",
            "No Improvement.\n",
            "Epoch  77/100 Batch  180/268 - Loss:  0.642, Seconds: 4.13\n",
            "Epoch  77/100 Batch  200/268 - Loss:  0.629, Seconds: 4.15\n",
            "Epoch  77/100 Batch  220/268 - Loss:  0.636, Seconds: 4.08\n",
            "Epoch  77/100 Batch  240/268 - Loss:  0.630, Seconds: 4.13\n",
            "Epoch  77/100 Batch  260/268 - Loss:  0.645, Seconds: 4.37\n",
            "Average loss for this update: 0.635\n",
            "No Improvement.\n",
            "Epoch  78/100 Batch   20/268 - Loss:  0.634, Seconds: 3.02\n",
            "Epoch  78/100 Batch   40/268 - Loss:  0.622, Seconds: 3.11\n",
            "Epoch  78/100 Batch   60/268 - Loss:  0.637, Seconds: 3.17\n",
            "Epoch  78/100 Batch   80/268 - Loss:  0.618, Seconds: 3.63\n",
            "Average loss for this update: 0.626\n",
            "No Improvement.\n",
            "Epoch  78/100 Batch  100/268 - Loss:  0.627, Seconds: 3.78\n",
            "Epoch  78/100 Batch  120/268 - Loss:  0.621, Seconds: 3.93\n",
            "Epoch  78/100 Batch  140/268 - Loss:  0.618, Seconds: 3.61\n",
            "Epoch  78/100 Batch  160/268 - Loss:  0.632, Seconds: 4.20\n",
            "Average loss for this update: 0.626\n",
            "No Improvement.\n",
            "Epoch  78/100 Batch  180/268 - Loss:  0.631, Seconds: 4.00\n",
            "Epoch  78/100 Batch  200/268 - Loss:  0.636, Seconds: 4.24\n",
            "Epoch  78/100 Batch  220/268 - Loss:  0.641, Seconds: 4.27\n",
            "Epoch  78/100 Batch  240/268 - Loss:  0.621, Seconds: 4.08\n",
            "Epoch  78/100 Batch  260/268 - Loss:  0.648, Seconds: 4.61\n",
            "Average loss for this update: 0.639\n",
            "No Improvement.\n",
            "Epoch  79/100 Batch   20/268 - Loss:  0.657, Seconds: 3.07\n",
            "Epoch  79/100 Batch   40/268 - Loss:  0.620, Seconds: 3.14\n",
            "Epoch  79/100 Batch   60/268 - Loss:  0.625, Seconds: 3.12\n",
            "Epoch  79/100 Batch   80/268 - Loss:  0.620, Seconds: 3.70\n",
            "Average loss for this update: 0.629\n",
            "No Improvement.\n",
            "Stopping Training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_4rooXoU3lP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b17149f9-a2a0-4cba-e2d9-ba073a4297b0"
      },
      "source": [
        "checkpoint = \"/content/drive/Model 1/Model 300/best_model.ckpt\" \n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "    names = []\n",
        "    [names.append(n.name) for n in loaded_graph.as_graph_def().node]\n",
        "names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/Model 1/Model 300/best_model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['input',\n",
              " 'targets',\n",
              " 'learning_rate',\n",
              " 'keep_prob',\n",
              " 'summary_length',\n",
              " 'Const',\n",
              " 'max_dec_len',\n",
              " 'text_length',\n",
              " 'ReverseV2/axis',\n",
              " 'ReverseV2',\n",
              " 'embedding_lookup/params_0',\n",
              " 'embedding_lookup/axis',\n",
              " 'embedding_lookup',\n",
              " 'embedding_lookup/Identity',\n",
              " 'encoder_0/DropoutWrapperInit/Const',\n",
              " 'encoder_0/DropoutWrapperInit/Const_1',\n",
              " 'encoder_0/DropoutWrapperInit_1/Const',\n",
              " 'encoder_0/DropoutWrapperInit_1/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Rank',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat/values_0',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/transpose',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/sequence_length',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Equal',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/All',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert/data_0',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert/data_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/CheckSeqLen',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/zeros',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Min',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Max',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/time',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArray',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArray_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/Shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Maximum/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Maximum',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Minimum',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/iteration_counter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less_1/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/LogicalAnd',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/LoopCond',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add/y',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/GreaterEqual/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/sub/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/Shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/truediv/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/truediv',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/Cast',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Assign',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/read',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Assign',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/read',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/split',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add/y',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add_1/y',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Rank_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range_1/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range_1/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_2/values_0',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_2/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/transpose_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/ReverseSequence',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Rank',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat/values_0',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/transpose',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/sequence_length',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Equal',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/All',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert/data_0',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert/data_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/CheckSeqLen',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/zeros',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Min',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Max',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/time',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArray',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArray_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/Shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Maximum/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Maximum',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Minimum',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/iteration_counter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less_1/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/LogicalAnd',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/LoopCond',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add/y',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/GreaterEqual/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/sub/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/Shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/truediv/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/truediv',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/Cast',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Assign',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/read',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Assign',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/read',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/split',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add/y',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add_1/y',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Rank_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range_1/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range_1/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_2/values_0',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_2/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/transpose_1',\n",
              " 'encoder_0/ReverseSequence',\n",
              " 'encoder_1/DropoutWrapperInit/Const',\n",
              " 'encoder_1/DropoutWrapperInit/Const_1',\n",
              " 'encoder_1/DropoutWrapperInit_1/Const',\n",
              " 'encoder_1/DropoutWrapperInit_1/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Rank',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat/values_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/transpose',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/sequence_length',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Equal',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/All',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert/data_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert/data_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/CheckSeqLen',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/zeros',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Min',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Max',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/time',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArray',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArray_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/Shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Maximum/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Maximum',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Minimum',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/iteration_counter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less_1/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/LogicalAnd',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/LoopCond',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/GreaterEqual/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/sub/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/Shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/truediv/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/truediv',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/Cast',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Assign',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/read',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Assign',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/read',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/split',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add_1/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Rank_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range_1/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range_1/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_2/values_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_2/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/transpose_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/ReverseSequence',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Rank',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat/values_0',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/transpose',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/sequence_length',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Equal',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/All',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert/data_0',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert/data_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/CheckSeqLen',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/zeros',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Min',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Max',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/time',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArray',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArray_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/Shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Maximum/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Maximum',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Minimum',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/iteration_counter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less_1/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/LogicalAnd',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/LoopCond',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add/y',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/GreaterEqual/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/sub/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/Shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/truediv/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/truediv',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/Cast',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Assign',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/read',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Assign',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/read',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/split',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add/y',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add_1/y',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Rank_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range_1/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range_1/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_2/values_0',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_2/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/transpose_1',\n",
              " 'encoder_1/ReverseSequence',\n",
              " 'concat/axis',\n",
              " 'concat',\n",
              " 'StridedSlice/begin',\n",
              " 'StridedSlice/end',\n",
              " 'StridedSlice/strides',\n",
              " 'StridedSlice',\n",
              " 'Fill/dims',\n",
              " 'Fill/value',\n",
              " 'Fill',\n",
              " 'concat_1/axis',\n",
              " 'concat_1',\n",
              " 'embedding_lookup_1/params_0',\n",
              " 'embedding_lookup_1/axis',\n",
              " 'embedding_lookup_1',\n",
              " 'embedding_lookup_1/Identity',\n",
              " 'decoder_0/DropoutWrapperInit/Const',\n",
              " 'decoder_0/DropoutWrapperInit/Const_1',\n",
              " 'decoder_1/DropoutWrapperInit/Const',\n",
              " 'decoder_1/DropoutWrapperInit/Const_1',\n",
              " 'BahdanauAttention/Shape',\n",
              " 'BahdanauAttention/strided_slice/stack',\n",
              " 'BahdanauAttention/strided_slice/stack_1',\n",
              " 'BahdanauAttention/strided_slice/stack_2',\n",
              " 'BahdanauAttention/strided_slice',\n",
              " 'BahdanauAttention/SequenceMask/Const',\n",
              " 'BahdanauAttention/SequenceMask/Const_1',\n",
              " 'BahdanauAttention/SequenceMask/Range',\n",
              " 'BahdanauAttention/SequenceMask/ExpandDims/dim',\n",
              " 'BahdanauAttention/SequenceMask/ExpandDims',\n",
              " 'BahdanauAttention/SequenceMask/Cast',\n",
              " 'BahdanauAttention/SequenceMask/Less',\n",
              " 'BahdanauAttention/SequenceMask/Cast_1',\n",
              " 'BahdanauAttention/ones/shape_as_tensor',\n",
              " 'BahdanauAttention/ones/Const',\n",
              " 'BahdanauAttention/ones',\n",
              " 'BahdanauAttention/Shape_1',\n",
              " 'BahdanauAttention/concat/axis',\n",
              " 'BahdanauAttention/concat',\n",
              " 'BahdanauAttention/Reshape',\n",
              " 'BahdanauAttention/mul',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/shape',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/min',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/max',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/sub',\n",
              " 'memory_layer/kernel/Initializer/random_uniform/mul',\n",
              " 'memory_layer/kernel/Initializer/random_uniform',\n",
              " 'memory_layer/kernel',\n",
              " 'memory_layer/kernel/Assign',\n",
              " 'memory_layer/kernel/read',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/axes',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/free',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Shape',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/GatherV2/axis',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/GatherV2',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/GatherV2_1/axis',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/GatherV2_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Const',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Prod',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Const_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Prod_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/concat/axis',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/concat',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/stack',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/transpose',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Reshape',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/transpose_1/perm',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/transpose_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Reshape_1/shape',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Reshape_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/MatMul',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/Const_2',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/concat_1/axis',\n",
              " 'BahdanauAttention/memory_layer/Tensordot/concat_1',\n",
              " 'BahdanauAttention/memory_layer/Tensordot',\n",
              " 'BahdanauAttention/Shape_2',\n",
              " 'BahdanauAttention/strided_slice_1/stack',\n",
              " 'BahdanauAttention/strided_slice_1/stack_1',\n",
              " 'BahdanauAttention/strided_slice_1/stack_2',\n",
              " 'BahdanauAttention/strided_slice_1',\n",
              " 'BahdanauAttention/Shape_3',\n",
              " 'BahdanauAttention/strided_slice_2/stack',\n",
              " 'BahdanauAttention/strided_slice_2/stack_1',\n",
              " 'BahdanauAttention/strided_slice_2/stack_2',\n",
              " 'BahdanauAttention/strided_slice_2',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_4',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_5',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_6',\n",
              " 'AttentionWrapperZeroState/DropoutWrapperZeroState/LSTMCellZeroState/Const_7',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/x',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Equal',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Const',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/All',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Const',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Const_1',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Const_2',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Const_3',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Assert/data_0',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Assert/data_1',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Assert/data_2',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Assert/data_4',\n",
              " 'AttentionWrapperZeroState/assert_equal_1/Assert/Assert',\n",
              " 'AttentionWrapperZeroState/checked_cell_state',\n",
              " 'AttentionWrapperZeroState/checked_cell_state_1',\n",
              " 'AttentionWrapperZeroState/Const',\n",
              " 'AttentionWrapperZeroState/ExpandDims/dim',\n",
              " 'AttentionWrapperZeroState/ExpandDims',\n",
              " 'AttentionWrapperZeroState/concat/axis',\n",
              " 'AttentionWrapperZeroState/concat',\n",
              " 'AttentionWrapperZeroState/zeros/Const',\n",
              " 'AttentionWrapperZeroState/zeros',\n",
              " 'AttentionWrapperZeroState/Const_1',\n",
              " 'AttentionWrapperZeroState/ExpandDims_1/dim',\n",
              " 'AttentionWrapperZeroState/ExpandDims_1',\n",
              " 'AttentionWrapperZeroState/zeros_1',\n",
              " 'AttentionWrapperZeroState/Const_2',\n",
              " 'AttentionWrapperZeroState/Const_3',\n",
              " 'AttentionWrapperZeroState/concat_1/axis',\n",
              " 'AttentionWrapperZeroState/concat_1',\n",
              " 'AttentionWrapperZeroState/zeros_2/Const',\n",
              " 'AttentionWrapperZeroState/zeros_2',\n",
              " 'AttentionWrapperZeroState/Const_4',\n",
              " 'AttentionWrapperZeroState/Const_5',\n",
              " 'AttentionWrapperZeroState/Const_6',\n",
              " 'AttentionWrapperZeroState/ExpandDims_2/dim',\n",
              " 'AttentionWrapperZeroState/ExpandDims_2',\n",
              " 'AttentionWrapperZeroState/concat_2/axis',\n",
              " 'AttentionWrapperZeroState/concat_2',\n",
              " 'AttentionWrapperZeroState/zeros_3/Const',\n",
              " 'AttentionWrapperZeroState/zeros_3',\n",
              " 'AttentionWrapperZeroState/Const_7',\n",
              " 'AttentionWrapperZeroState/ExpandDims_3/dim',\n",
              " 'AttentionWrapperZeroState/ExpandDims_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/actual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/Size/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/Size',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/is_rank/actual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/is_rank',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/Equal/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/Equal/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/Equal',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/equal_1/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/equal_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/exclude_partial_shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape/shape_tensor_equal',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/is_shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/Assert/data_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/Assert/data_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/assert_shape/Assert',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3/Identity',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4/assert_shape/actual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4/assert_shape/is_shape/Size/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4/assert_shape/is_shape/Size',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4/assert_shape/is_shape/is_rank/actual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4/assert_shape/is_shape/is_rank',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLTgl3MT6ml8"
      },
      "source": [
        "## Making Our Own Summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycJAqueX6ml9"
      },
      "source": [
        "To see the quality of the summaries that this model can generate, you can either create your own review, or use a review from the dataset. You can set the length of the summary to a fixed value, or use a random value like I have here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJsdh6eG6ml-"
      },
      "source": [
        "def text_to_seq(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    \n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2Pbp2Cu6mmE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "0c35de0a-f4b4-4017-cd0d-94685ca85fc4"
      },
      "source": [
        "input_sentence = \"office depot is branching into coworking space opening its first workonomy hub in its los gatos california store the firm s workonomy concept is targeted at smes a range of services are expected to roll out to 1 000 stores by end august with 141 stores lined up to host tech support kiosks while rival staples charges 130 month membership to tap into its coworking offering which launched in partnership with workbar in 2016 office depot s prices range from 40 day to 750 month for a private office\\\n",
        "                office depot stores to host its workonomy coworking spaces.\"\n",
        "text = text_to_seq(input_sentence)\n",
        "random = np.random.randint(0,len(clean_texts))\n",
        "input_sentence = clean_texts[random]\n",
        "text = text_to_seq(clean_texts[random])\n",
        "checkpoint = \"/content/drive/Model 1/Model 300/best_model.ckpt\" \n",
        "\n",
        "\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding from the tweet\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', news.Text[random])\n",
        "print('Original summary:', news.Summary[random])#clean_summaries[random]\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/Model 1/Model 300/best_model.ckpt\n",
            "Original Text: VTB, Sberbank and the Bank of Moscow have announced they became the target of an extensive denial-of-service (DDoS) attack last autumn. The extortionists demanded 50 Bitcoins in return for stopping the attacks at a total value of around $11,500. Whilst Russia's three largest banks did not agree to pay the hackers, the banks have not experienced large-scale DDoS attacks since and did not suffer any damage as a result of the attack.\n",
            "\n",
            "Original summary: Russia’s top 3 banks were target of world’s largest DDoS attack\n",
            "\n",
            "\n",
            "Text\n",
            "  Word Ids:    [4756, 1938, 211, 5877, 1641, 20180, 982, 14558, 16744, 997, 10783, 2025, 3793, 2260, 31321, 7206, 271, 2155, 793, 6907, 26, 1470, 936, 2100, 1002, 818, 9901, 2993, 265, 671, 1440, 2622, 207, 2126, 1440, 3260, 1406, 1232, 10783, 26, 3484, 3736, 4485, 4298, 2025]\n",
            "  Input Words: vtb sberbank bank moscow announced became target extensive denial service ddos attack last autumn extortionists demanded 50 bitcoins return stopping attacks total value around 11 500 whilst russia three largest banks agree pay hackers banks experienced large scale ddos attacks since suffer damage result attack\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [4562, 293, 835, 1440, 19, 1219]\n",
            "  Response Words: russia’s top 3 banks to hit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xko7rtfk8TOR"
      },
      "source": [
        "## Bleu Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmoclVpc5ZBL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "fc5af0ef-e7fe-42a7-9411-8028f853b7a0"
      },
      "source": [
        "import nltk\n",
        "\n",
        "hypothesis =  ['struggling', 'latin', 'america', 'hit','its','multiple','risk' ]     \n",
        "reference = ['Struggling' ,'Latin', 'America', 'multiple' , 'further' , 'shocks'  ]  \n",
        "#there may be several references\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
        "print(BLEUscore)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6147881529512643\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhAIbFJQ8i7x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "736b96d0-ddbf-4b9b-8633-d301a48649f5"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = ['Facebook', 'stays', 'popular', 'with','publishers','despite','poor', 'transparency' ,'at', 'risk' ]     \n",
        "candidate = ['facebook' ,'depot', 'flexible', 'office' , 'working'  ]       \n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p_CRprLBo2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "f8c258eb-23d8-4f29-eb73-60cbb89ddfb5"
      },
      "source": [
        "!pip install sumeval\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sumeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/87/bfc0f9397b9421305863edfdd2dbea637e47204976cb5473535c856338f4/sumeval-0.2.2.tar.gz (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: plac>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from sumeval) (1.1.3)\n",
            "Collecting sacrebleu>=1.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/4b/6c7a0b26a48d88f56573d11aa5058808fe0d36ba40951287894f943556b5/sacrebleu-1.4.10-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: sumeval\n",
            "  Building wheel for sumeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sumeval: filename=sumeval-0.2.2-cp36-none-any.whl size=54535 sha256=37da8e3b1aa49a20c49700dd5eb7738e368d2302c456daf34a14034fc31c082a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/6f/57/19ceecab21445c88f3c565735fa1887b4cd18d340c972eb445\n",
            "Successfully built sumeval\n",
            "Installing collected packages: portalocker, sacrebleu, sumeval\n",
            "Successfully installed portalocker-1.7.0 sacrebleu-1.4.10 sumeval-0.2.2\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (47.1.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jmnDRhhBzRL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6651d2a8-4623-4831-c40a-cb55962fa404"
      },
      "source": [
        "#https://github.com/chakki-works/sumeval\n",
        "#https://github.com/Tian312/awesome-text-summarization\n",
        "\n",
        "from sumeval.metrics.rouge import RougeCalculator\n",
        "\n",
        "refrence_summary = \"Struggling Latin America faces multiple further shocks\"\n",
        "model_summary = \"struggling latin america hit multiple risk\"\n",
        "\n",
        "rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
        "\n",
        "rouge_1 = rouge.rouge_n(\n",
        "            summary=model_summary,\n",
        "            references=refrence_summary,\n",
        "            n=1)\n",
        "\n",
        "rouge_2 = rouge.rouge_n(\n",
        "            summary=model_summary,\n",
        "            references=[refrence_summary],\n",
        "            n=2)\n",
        "\n",
        "rouge_l = rouge.rouge_l(\n",
        "            summary=model_summary,\n",
        "            references=[refrence_summary])\n",
        "\n",
        "# You need spaCy to calculate ROUGE-BE\n",
        "\n",
        "rouge_be = rouge.rouge_be(\n",
        "            summary=model_summary,\n",
        "            references=[refrence_summary])\n",
        "\n",
        "print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(\n",
        "    rouge_1, rouge_2, rouge_l, rouge_be\n",
        ").replace(\", \", \"\\n\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b.multiple=(amod)=>risk\n",
            "a.risk=(dobj)=>hit\n",
            "<BasicElement: risk-[dobj]->hit>\n",
            "b.further=(amod)=>shocks\n",
            "a.shocks=(dobj)=>faces\n",
            "<BasicElement: shocks-[dobj]->face>\n",
            "ROUGE-1: 0.6666666666666666\n",
            "ROUGE-2: 0.4000000000000001\n",
            "ROUGE-L: 0.6666666666666666\n",
            "ROUGE-BE: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JMgn0CeKDVD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5b859087-f705-40de-ea27-298989de3a97"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "sentence = (\"\"\"office depot is branching into coworking space opening its first workonomy hub in its los gatos california store the firm s workonomy concept is targeted at smes a range of services are expected to roll out to 1 000 stores by end august with 141 stores lined up to host tech support kiosks while rival staples charges 130 month membership to tap into its coworking offering which launched in partnership with workbar in 2016 office depot s prices range from 40 day to 750 month for a private office.\n",
        "\"\"\",\"\")\n",
        "model = (\"office depot stores to host its workonomy coworking spaces\",\"\")\n",
        "summary = (\"office supply centre to expand its\",\"\")\n",
        "\n",
        "tfidf_matrix_sentence = tfidf_vectorizer.fit_transform(sentence) \n",
        "tfidf_matrix_model = tfidf_vectorizer.transform(model) \n",
        "tfidf_matrix_summary = tfidf_vectorizer.transform(summary) \n",
        "\n",
        "\n",
        "cosine = cosine_similarity(tfidf_matrix_sentence, tfidf_matrix_model)\n",
        "print(cosine)\n",
        "\n",
        "cosine = cosine_similarity(tfidf_matrix_sentence, tfidf_matrix_summary)\n",
        "print(cosine)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.5976143 0.       ]\n",
            " [0.        0.       ]]\n",
            "[[0.53674504 0.        ]\n",
            " [0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU9ZbhuB6mmJ"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFDpGZlk6mmK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}